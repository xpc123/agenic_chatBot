#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŠŸèƒ½æµ‹è¯•è¿è¡Œå™¨

ä¸€é”®è¿è¡Œæ‰€æœ‰åŠŸèƒ½æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/run_functional_tests.py                    # è¿è¡Œå…¨éƒ¨æµ‹è¯•
    python scripts/run_functional_tests.py --quick            # å¿«é€Ÿæµ‹è¯•
    python scripts/run_functional_tests.py --category api     # åªæµ‹è¯• API
    python scripts/run_functional_tests.py --report           # ç”Ÿæˆ HTML æŠ¥å‘Š
"""
import subprocess
import sys
import os
import time
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def check_backend():
    """æ£€æŸ¥åç«¯æ˜¯å¦è¿è¡Œ"""
    import requests
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        return response.status_code in [200, 503]
    except:
        return False


def run_pytest(test_files, extra_args=None):
    """è¿è¡Œ pytest"""
    cmd = ["python", "-m", "pytest"]
    cmd.extend(test_files)
    cmd.extend(["-v", "--tb=short"])
    
    if extra_args:
        cmd.extend(extra_args)
    
    result = subprocess.run(cmd, cwd=PROJECT_ROOT)
    return result.returncode == 0


def run_all_functional_tests(quick=False, category=None, report=False):
    """è¿è¡Œæ‰€æœ‰åŠŸèƒ½æµ‹è¯•"""
    print("=" * 70)
    print("ğŸ§ª åŠŸèƒ½æµ‹è¯•è¿è¡Œå™¨")
    print("=" * 70)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # æ£€æŸ¥åç«¯
    print("ğŸ” æ£€æŸ¥åç«¯æœåŠ¡...")
    if not check_backend():
        print("âŒ åç«¯æœåŠ¡æœªè¿è¡Œï¼")
        print("è¯·å…ˆå¯åŠ¨åç«¯: cd backend && python run.py")
        return False
    print("âœ… åç«¯æœåŠ¡æ­£å¸¸")
    print()
    
    # æµ‹è¯•æ–‡ä»¶åˆ†ç±»
    test_categories = {
        "api": [
            "tests/functional/test_all_api_endpoints.py",
        ],
        "edge": [
            "tests/functional/test_edge_cases.py",
        ],
        "performance": [
            "tests/functional/test_performance.py",
        ],
        "scenarios": [
            "tests/functional/test_chat_scenarios.py",
        ],
    }
    
    # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•
    if category:
        if category not in test_categories:
            print(f"âŒ æœªçŸ¥ç±»åˆ«: {category}")
            print(f"å¯ç”¨ç±»åˆ«: {list(test_categories.keys())}")
            return False
        test_files = test_categories[category]
    else:
        test_files = []
        for files in test_categories.values():
            test_files.extend(files)
    
    # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    existing_files = [f for f in test_files if (PROJECT_ROOT / f).exists()]
    
    if not existing_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
        return False
    
    print(f"ğŸ“‹ å°†è¿è¡Œ {len(existing_files)} ä¸ªæµ‹è¯•æ–‡ä»¶:")
    for f in existing_files:
        print(f"   - {f}")
    print()
    
    # é¢å¤–å‚æ•°
    extra_args = []
    if quick:
        extra_args.extend(["-x", "--timeout=30"])  # å¤±è´¥å³åœï¼Œ30ç§’è¶…æ—¶
    
    if report:
        report_dir = PROJECT_ROOT / "test_reports"
        report_dir.mkdir(exist_ok=True)
        report_file = report_dir / f"functional_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        extra_args.extend([f"--html={report_file}", "--self-contained-html"])
    
    # è¿è¡Œæµ‹è¯•
    print("ğŸš€ å¼€å§‹è¿è¡Œæµ‹è¯•...")
    print("-" * 70)
    
    start_time = time.time()
    success = run_pytest(existing_files, extra_args)
    elapsed = time.time() - start_time
    
    print("-" * 70)
    print()
    
    if success:
        print(f"âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! (è€—æ—¶: {elapsed:.1f}s)")
    else:
        print(f"âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ (è€—æ—¶: {elapsed:.1f}s)")
    
    if report:
        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Š: {report_file}")
    
    return success


def run_evaluation(categories=None):
    """è¿è¡Œè¯„ä¼°æµ‹è¯•"""
    print("=" * 70)
    print("ğŸ“Š èƒ½åŠ›è¯„ä¼°è¿è¡Œå™¨")
    print("=" * 70)
    
    # æ£€æŸ¥åç«¯
    if not check_backend():
        print("âŒ åç«¯æœåŠ¡æœªè¿è¡Œï¼")
        return False
    
    # è¿è¡Œç»¼åˆè¯„ä¼°
    cmd = ["python", "-m", "tests.evaluation.comprehensive_evaluation"]
    if categories:
        cmd.extend(["--categories"] + categories)
    
    result = subprocess.run(cmd, cwd=PROJECT_ROOT)
    return result.returncode == 0


def print_summary():
    """æ‰“å°æµ‹è¯•è¦†ç›–æ‘˜è¦"""
    print()
    print("=" * 70)
    print("ğŸ“‹ åŠŸèƒ½æµ‹è¯•è¦†ç›–æ‘˜è¦")
    print("=" * 70)
    print("""
æµ‹è¯•æ–‡ä»¶:
  1. test_all_api_endpoints.py - å®Œæ•´ API ç«¯ç‚¹æµ‹è¯•
     - Chat API (10 ä¸ªç«¯ç‚¹)
     - Documents API (4 ä¸ªç«¯ç‚¹)
     - Settings API (10 ä¸ªç«¯ç‚¹)
     - Tools API (5 ä¸ªç«¯ç‚¹)
     - Batch API (2 ä¸ªç«¯ç‚¹)
     - åœºæ™¯æµ‹è¯• (å¤šè½®å¯¹è¯ã€æ–‡æ¡£å·¥ä½œæµã€è®¾ç½®å·¥ä½œæµ)

  2. test_edge_cases.py - è¾¹ç•Œæ¡ä»¶æµ‹è¯•
     - è¾“å…¥è¾¹ç•Œ (ç©ºã€è¶…é•¿ã€Unicodeã€ç‰¹æ®Šå­—ç¬¦)
     - ä¼šè¯ç®¡ç†è¾¹ç•Œ
     - API è¯·æ±‚æ ¼å¼
     - å¹¶å‘æµ‹è¯•
     - å®‰å…¨æ€§æµ‹è¯• (SQL æ³¨å…¥ã€XSSã€è·¯å¾„éå†)
     - è¶…æ—¶å’Œé‡è¯•
     - æ„å›¾åˆ†æè¾¹ç•Œ
     - æ–‡æ¡£æ“ä½œè¾¹ç•Œ

  3. test_performance.py - æ€§èƒ½æµ‹è¯•
     - ç«¯ç‚¹æ€§èƒ½åŸºå‡†
     - ååé‡æµ‹è¯•
     - å»¶è¿Ÿç¨³å®šæ€§æµ‹è¯•
     - èµ„æºæ•æ„Ÿæ€§æµ‹è¯•
     - æµå¼å“åº”æ€§èƒ½

  4. test_chat_scenarios.py - å¯¹è¯åœºæ™¯æµ‹è¯•
     - åŸºç¡€å¯¹è¯
     - å¤šè½®å¯¹è¯
     - æ„å›¾è¯†åˆ«
     - å·¥å…·è°ƒç”¨
     - æ–‡æ¡£ç®¡ç†
     - æŠ€èƒ½ç³»ç»Ÿ
     - é”™è¯¯å¤„ç†
     - æµå¼å“åº”
     - æ‰¹é‡æ“ä½œ

è¯„ä¼°æ¡†æ¶:
  - chatbot_evaluation.py - æ ‡å‡†è¯„ä¼° (9 ä¸ªç”¨ä¾‹)
  - comprehensive_evaluation.py - ç»¼åˆè¯„ä¼° (35+ ä¸ªç”¨ä¾‹)
    - æ„å›¾è¯†åˆ«è¯„ä¼°
    - å¤šè½®å¯¹è¯è¯„ä¼°
    - ä»£ç èƒ½åŠ›è¯„ä¼°
    - çŸ¥è¯†é—®ç­”è¯„ä¼°
    - è¾¹ç•Œæƒ…å†µè¯„ä¼°
    - å“åº”è´¨é‡è¯„ä¼°
""")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åŠŸèƒ½æµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--category", choices=["api", "edge", "performance", "scenarios"],
                       help="åªè¿è¡Œç‰¹å®šç±»åˆ«çš„æµ‹è¯•")
    parser.add_argument("--report", action="store_true", help="ç”Ÿæˆ HTML æŠ¥å‘Š")
    parser.add_argument("--evaluate", action="store_true", help="è¿è¡Œèƒ½åŠ›è¯„ä¼°")
    parser.add_argument("--eval-categories", nargs="+",
                       choices=["standard", "intent", "multi_turn", "code", 
                               "knowledge", "edge", "quality"],
                       help="è¯„ä¼°ç±»åˆ«")
    parser.add_argument("--summary", action="store_true", help="æ˜¾ç¤ºæµ‹è¯•è¦†ç›–æ‘˜è¦")
    
    args = parser.parse_args()
    
    if args.summary:
        print_summary()
        sys.exit(0)
    
    if args.evaluate:
        success = run_evaluation(args.eval_categories)
    else:
        success = run_all_functional_tests(
            quick=args.quick,
            category=args.category,
            report=args.report
        )
    
    sys.exit(0 if success else 1)


