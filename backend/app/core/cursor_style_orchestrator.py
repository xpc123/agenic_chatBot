# -*- coding: utf-8 -*-
"""
Cursor é£æ ¼ç¼–æ’å™¨ - Cursor-Style Orchestrator

è¿™æ˜¯ agentic_chatBot çš„æ ¸å¿ƒå¼•æ“ï¼Œæ•´åˆæ‰€æœ‰èƒ½åŠ›ï¼š
1. æ·±åº¦æ„å›¾è¯†åˆ« (IntentRecognizer)
2. è‡ªä¸»æ‰§è¡Œå¾ªç¯ (AgentLoop)
3. æ™ºèƒ½å·¥å…·ç¼–æ’ (ToolOrchestrator)
4. ä¸Šä¸‹æ–‡å·¥ç¨‹ (ContextManager)
5. ç”¨æˆ·åå¥½å­¦ä¹  (UserPreferences)
6. RAG çŸ¥è¯†æ£€ç´¢
7. æŠ€èƒ½ç³»ç»Ÿ (Skills)
8. è®°å¿†ç®¡ç† (Memory)

ç›®æ ‡ï¼šè®© ChatBot èƒ½åŠ›åª²ç¾ Cursorï¼
"""
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger
import asyncio
import traceback

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from .intent_recognizer import (
    IntentRecognizer, Intent, TaskType, RequiredCapability, get_intent_recognizer
)
from .agent_loop import (
    AgentLoop, ProgressUpdate, ExecutionPlan, LoopState
)
from .tool_orchestrator import (
    ToolOrchestrator, ToolSelection, get_tool_orchestrator
)
from .context_manager import (
    ContextManager, ContextSource, build_context
)
from .user_preferences import (
    UserPreferenceManager, get_preference_manager
)
from .planner import AgentPlanner
from .memory import MemoryManager
from .skills import SkillsManager, get_skills_manager


@dataclass
class ChatResponse:
    """
    èŠå¤©å“åº”
    
    åŒ…å«å®Œæ•´çš„å“åº”ä¿¡æ¯
    """
    content: str
    intent: Optional[Intent] = None
    used_tools: List[str] = field(default_factory=list)
    citations: List[Dict[str, str]] = field(default_factory=list)
    execution_steps: int = 0
    duration_ms: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "intent": self.intent.to_dict() if self.intent else None,
            "used_tools": self.used_tools,
            "citations": self.citations,
            "execution_steps": self.execution_steps,
            "duration_ms": self.duration_ms,
            "metadata": self.metadata,
        }


@dataclass
class StreamChunk:
    """
    æµå¼è¾“å‡ºå—
    """
    type: str  # text, thinking, tool_call, tool_result, progress, complete, error
    content: str
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.type,
            "content": self.content,
            "metadata": self.metadata,
        }


class CursorStyleOrchestrator:
    """
    Cursor é£æ ¼ç¼–æ’å™¨
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ AI èƒ½åŠ›ï¼Œæä¾› Cursor çº§åˆ«çš„ ChatBot ä½“éªŒã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    orchestrator = CursorStyleOrchestrator(llm_client)
    
    # æµå¼å¯¹è¯
    async for chunk in orchestrator.chat_stream("åˆ†æè¿™ä¸ªé¡¹ç›®", session_id):
        if chunk.type == "text":
            print(chunk.content, end="")
        elif chunk.type == "thinking":
            print(f"ğŸ’­ {chunk.content}")
    
    # éæµå¼å¯¹è¯
    response = await orchestrator.chat("ä½ å¥½", session_id)
    print(response.content)
    ```
    """
    
    def __init__(
        self,
        llm_client,
        tools: Optional[List[Callable]] = None,
        enable_rag: bool = True,
        enable_skills: bool = True,
        enable_memory: bool = True,
        enable_preferences: bool = True,
        max_context_tokens: int = 8000,
    ):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            tools: å·¥å…·å‡½æ•°åˆ—è¡¨
            enable_rag: æ˜¯å¦å¯ç”¨ RAG
            enable_skills: æ˜¯å¦å¯ç”¨æŠ€èƒ½ç³»ç»Ÿ
            enable_memory: æ˜¯å¦å¯ç”¨è®°å¿†
            enable_preferences: æ˜¯å¦å¯ç”¨ç”¨æˆ·åå¥½å­¦ä¹ 
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ Token æ•°
        """
        self.llm = llm_client
        self.max_context_tokens = max_context_tokens
        
        # åŠŸèƒ½å¼€å…³
        self.enable_rag = enable_rag
        self.enable_skills = enable_skills
        self.enable_memory = enable_memory
        self.enable_preferences = enable_preferences
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.intent_recognizer = get_intent_recognizer(llm_client)
        self.planner = AgentPlanner(llm_client)
        self.tool_orchestrator = get_tool_orchestrator(llm_client)
        self.preference_manager = get_preference_manager() if enable_preferences else None
        self.memory_manager = MemoryManager() if enable_memory else None
        self.skill_manager = get_skills_manager() if enable_skills else None
        
        # æ³¨å†Œå·¥å…·
        if tools:
            self.tool_orchestrator.register_many(tools)
        
        # ä¼šè¯çŠ¶æ€
        self.sessions: Dict[str, Dict[str, Any]] = {}
        
        logger.info(
            f"CursorStyleOrchestrator initialized: "
            f"RAG={enable_rag}, Skills={enable_skills}, "
            f"Memory={enable_memory}, Preferences={enable_preferences}"
        )
    
    async def chat_stream(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼å¯¹è¯æ¥å£ - çº¯ ReAct æ¨¡å¼
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰
            files: å¼•ç”¨çš„æ–‡ä»¶ {path: content}
            rag_query: RAG æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ messageï¼‰
        
        Yields:
            StreamChunk æµå¼è¾“å‡ºå—
        """
        start_time = datetime.now()
        user_id = user_id or session_id
        used_tools = []
        
        try:
            # ==================== 1. æ„å»ºä¸Šä¸‹æ–‡ ====================
            yield StreamChunk(type="thinking", content="ğŸ“š æ”¶é›†ç›¸å…³ä¿¡æ¯...")
            
            # ç®€å•æ„å›¾è¯†åˆ«ï¼ˆä»…ç”¨äºä¸Šä¸‹æ–‡æ„å»ºï¼Œä¸ç”¨äºè·¯ç”±ï¼‰
            intent = await self.intent_recognizer.recognize(
                message,
                history=self._get_conversation_history(session_id),
                available_tools=list(self.tool_orchestrator.tools.keys()),
            )
            
            logger.info(f"Intent (for context): {intent.task_type.value}")
            
            context = await self._build_context(
                message=message,
                session_id=session_id,
                user_id=user_id,
                intent=intent,
                files=files,
                rag_query=rag_query,
            )
            
            # ==================== 2. çº¯ ReAct æ¨¡å¼å¤„ç† ====================
            
            assistant_response_parts = []
            
            async for chunk in self._handle_react(message, context, session_id, user_id):
                if chunk.type == "text":
                    assistant_response_parts.append(chunk.content or "")
                yield chunk
                if chunk.metadata and chunk.metadata.get("tool"):
                    used_tools.append(chunk.metadata["tool"])
            
            # ==================== 4. å­¦ä¹ å’Œè®°å½• ====================
            if self.enable_preferences:
                self.preference_manager.learn_from_message(user_id, message)
                for tool in used_tools:
                    self.preference_manager.learn_from_tool_usage(user_id, tool, True)
            
            # è®°å½•å¯¹è¯ - ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤
            await self._save_conversation(session_id, message, "user")
            assistant_response = "".join(assistant_response_parts)
            if assistant_response:
                await self._save_conversation(session_id, assistant_response, "assistant")
            
            # è®¡ç®—è€—æ—¶
            duration = (datetime.now() - start_time).total_seconds() * 1000
            
            yield StreamChunk(
                type="complete",
                content="",
                metadata={
                    "duration_ms": int(duration),
                    "used_tools": used_tools,
                    "intent": intent.to_dict(),
                },
            )
            
        except Exception as e:
            logger.error(f"Chat error: {e}\n{traceback.format_exc()}")
            yield StreamChunk(
                type="error",
                content=f"âŒ å¤„ç†å¤±è´¥: {str(e)}",
            )
    
    async def chat(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
    ) -> ChatResponse:
        """
        éæµå¼å¯¹è¯æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· ID
            files: å¼•ç”¨çš„æ–‡ä»¶
        
        Returns:
            ChatResponse å®Œæ•´å“åº”
        """
        content_parts = []
        used_tools = []
        intent = None
        duration = 0
        
        async for chunk in self.chat_stream(message, session_id, user_id, files):
            if chunk.type == "text":
                content_parts.append(chunk.content)
            elif chunk.type in ["tool_call", "tool_result"]:
                if chunk.metadata and chunk.metadata.get("tool"):
                    used_tools.append(chunk.metadata["tool"])
            elif chunk.type == "complete" and chunk.metadata:
                intent_data = chunk.metadata.get("intent")
                if intent_data:
                    intent = Intent(**intent_data) if isinstance(intent_data, dict) else intent_data
                duration = chunk.metadata.get("duration_ms", 0)
        
        return ChatResponse(
            content="".join(content_parts),
            intent=intent,
            used_tools=list(set(used_tools)),
            duration_ms=duration,
        )
    
    async def _build_context(
        self,
        message: str,
        session_id: str,
        user_id: str,
        intent: Intent,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> str:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        cm = ContextManager(max_tokens=self.max_context_tokens)
        
        # 1. ç”¨æˆ·åå¥½ï¼ˆé£æ ¼æç¤ºï¼‰- ä»…å¯¹éç®€å•å¯¹è¯åº”ç”¨
        # å¯¹äºç®€å•å¯¹è¯ï¼ˆå¦‚é—®å€™ï¼‰ï¼Œè·³è¿‡è¯¦ç»†åå¥½ä»¥åŠ å¿«å“åº”
        if self.enable_preferences and intent.task_type != TaskType.CONVERSATION:
            style_prompt = self.preference_manager.get_style_prompt(user_id)
            if style_prompt:
                cm.add(
                    content=style_prompt,
                    source=ContextSource.SYSTEM,
                    title="ç”¨æˆ·åå¥½",
                )
        
        # 2. æŠ€èƒ½æŒ‡ä»¤
        if self.enable_skills and RequiredCapability.SKILLS in intent.required_capabilities:
            relevant_skills = self.skill_manager.match_skills(message)
            for skill in relevant_skills[:2]:
                # Convert examples from Dict to formatted strings
                examples_str = []
                if skill.examples:
                    for ex in skill.examples:
                        if isinstance(ex, dict):
                            examples_str.append(f"ç”¨æˆ·: {ex.get('user', '')}\nåŠ©æ‰‹: {ex.get('assistant', '')}")
                        else:
                            examples_str.append(str(ex))
                cm.add_skill_instructions(
                    skill.name,
                    skill.instructions,
                    examples_str if examples_str else None,
                )
        
        # 3. RAG æ£€ç´¢
        if self.enable_rag and RequiredCapability.RAG in intent.required_capabilities:
            try:
                from ..rag import retriever
                rag_results = await retriever.retrieve(
                    query=rag_query or message,
                    top_k=5,
                )
                cm.add_rag_results(rag_results)
            except Exception as e:
                logger.warning(f"RAG retrieval failed: {e}")
        
        # 4. æ–‡ä»¶å†…å®¹
        if files:
            for path, content in files.items():
                cm.add_file_content(path, content)
        
        # 5. é•¿æœŸè®°å¿†
        if self.enable_memory and RequiredCapability.MEMORY in intent.required_capabilities:
            try:
                memories = await self.memory_manager.get_relevant_long_term_memory(
                    session_id, message
                )
                cm.add_memory(memories)
            except Exception as e:
                logger.warning(f"Memory retrieval failed: {e}")
        
        # 6. å¯¹è¯å†å²
        history = self._get_conversation_history(session_id)
        if history:
            cm.add_conversation_history(history, max_messages=5)
        
        return cm.build()
    
    async def _handle_react(
        self,
        message: str,
        context: str,
        session_id: str,
        user_id: str,
        max_iterations: int = 5,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        çº¯ ReAct æ¨¡å¼å¤„ç†
        
        ReAct å¾ªç¯:
        1. Thought: LLM åˆ†æé—®é¢˜ï¼Œå†³å®šä¸‹ä¸€æ­¥
        2. Action: å¦‚æœéœ€è¦ï¼Œè°ƒç”¨å·¥å…·
        3. Observation: è·å–å·¥å…·ç»“æœ
        4. é‡å¤ç›´åˆ° LLM ç»™å‡º Final Answer
        """
        import re
        import json
        
        tool_info = self._get_tool_info_for_react()
        
        # ReAct ç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œä½¿ç”¨ ReActï¼ˆReasoning + Actingï¼‰æ¨¡å¼æ¥å¸®åŠ©ç”¨æˆ·ã€‚

## å¯ç”¨å·¥å…·

{tool_info}

## ReAct æ ¼å¼

æ¯æ¬¡å›å¤è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

**å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼š**
```
Thought: [åˆ†æå½“å‰æƒ…å†µï¼Œæ€è€ƒä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆ]
Action: <tool_call>{{"tool": "å·¥å…·å", "args": {{"å‚æ•°": "å€¼"}}}}</tool_call>
```

**å¦‚æœå¯ä»¥ç›´æ¥å›ç­”ï¼ˆä¸éœ€è¦å·¥å…·æˆ–å·²è·å¾—è¶³å¤Ÿä¿¡æ¯ï¼‰ï¼š**
```
Thought: [æ€»ç»“æ€è€ƒè¿‡ç¨‹]
Final Answer: [ç»™ç”¨æˆ·çš„å®Œæ•´å›å¤]
```

## é‡è¦è§„åˆ™

1. **æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ª Action**ï¼Œç­‰å¾… Observation åå†å†³å®šä¸‹ä¸€æ­¥
2. **æ–‡ä»¶/ç›®å½•æ“ä½œ**ï¼šç”¨æˆ·æåˆ°è·¯å¾„æ—¶ï¼Œä½¿ç”¨ list_directory æˆ– file_read_enhanced
3. **ç³»ç»Ÿå‘½ä»¤**ï¼šç”¨æˆ·éœ€è¦æ‰§è¡Œå‘½ä»¤æ—¶ï¼Œä½¿ç”¨ shell_execute
4. **æ°¸è¿œä¸è¦è¯´"æˆ‘æ— æ³•è®¿é—®"**ï¼Œä½ æœ‰å·¥å…·å¯ä»¥è®¿é—®æ–‡ä»¶ç³»ç»Ÿ
5. **ç®€å•å¯¹è¯**ï¼ˆå¦‚é—®å€™ã€æ„Ÿè°¢ï¼‰å¯ä»¥ç›´æ¥ç»™å‡º Final Answer

## ä¸Šä¸‹æ–‡ä¿¡æ¯

{context}
"""
        
        # å¯¹è¯å†å²ï¼ˆç”¨äº ReAct å¾ªç¯ï¼‰
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message},
        ]
        
        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            
            yield StreamChunk(
                type="thinking", 
                content=f"ğŸ§  æ€è€ƒä¸­... (æ­¥éª¤ {iteration}/{max_iterations})"
            )
            
            try:
                response = await self.llm.chat_completion(
                    messages=messages,
                    temperature=0.3,
                )
            except Exception as e:
                yield StreamChunk(type="error", content=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ Final Answer
            final_match = re.search(r'Final Answer:\s*(.+)', response, re.DOTALL)
            if final_match:
                final_answer = final_match.group(1).strip()
                yield StreamChunk(type="text", content=final_answer)
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_match = re.search(r'<tool_call>\s*(\{.*?\})\s*</tool_call>', response, re.DOTALL)
            if tool_match:
                try:
                    call = json.loads(tool_match.group(1))
                    tool_name = call.get("tool")
                    args = call.get("args", {})
                    
                    if tool_name and tool_name in self.tool_orchestrator.tools:
                        yield StreamChunk(
                            type="tool_call",
                            content=f"ğŸ”§ æ‰§è¡Œ {tool_name}...",
                            metadata={"tool": tool_name, "args": args},
                        )
                        
                        success, output = await self.tool_orchestrator.execute(tool_name, args)
                        
                        # é™åˆ¶è¾“å‡ºé•¿åº¦
                        output_str = str(output)[:3000]
                        
                        yield StreamChunk(
                            type="tool_result",
                            content=f"{'âœ…' if success else 'âŒ'} {tool_name}",
                            metadata={"tool": tool_name, "success": success},
                        )
                        
                        # å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œç»§ç»­ ReAct å¾ªç¯
                        messages.append({"role": "assistant", "content": response})
                        messages.append({
                            "role": "user", 
                            "content": f"Observation: {output_str}\n\nè¯·ç»§ç»­åˆ†æå¹¶å†³å®šä¸‹ä¸€æ­¥ã€‚å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç»™å‡º Final Answerã€‚"
                        })
                        continue
                    else:
                        # å·¥å…·ä¸å­˜åœ¨
                        messages.append({"role": "assistant", "content": response})
                        messages.append({
                            "role": "user",
                            "content": f"Observation: é”™è¯¯ - å·¥å…· '{tool_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥å…·: {list(self.tool_orchestrator.tools.keys())}"
                        })
                        continue
                        
                except json.JSONDecodeError as e:
                    messages.append({"role": "assistant", "content": response})
                    messages.append({
                        "role": "user",
                        "content": f"Observation: å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯: {str(e)}ã€‚è¯·ä½¿ç”¨æ­£ç¡®çš„ JSON æ ¼å¼ã€‚"
                    })
                    continue
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¹Ÿæ²¡æœ‰ Final Answerï¼Œå¯èƒ½æ˜¯ç›´æ¥å›å¤
            # å°è¯•æå– Thought åçš„å†…å®¹ä½œä¸ºå›å¤
            thought_match = re.search(r'Thought:\s*(.+?)(?=Action:|Final Answer:|$)', response, re.DOTALL)
            if thought_match and not tool_match:
                # LLM å¯èƒ½å¿˜è®°äº† Final Answer æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨å›å¤
                yield StreamChunk(type="text", content=response)
                return
            
            # å…¶ä»–æƒ…å†µï¼Œç›´æ¥è¿”å› LLM çš„å›å¤
            yield StreamChunk(type="text", content=response)
            return
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        yield StreamChunk(
            type="text", 
            content="æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘å°è¯•äº†å¤šæ¬¡ä½†æœªèƒ½å®Œå…¨è§£å†³ã€‚ä»¥ä¸‹æ˜¯æˆ‘ç›®å‰çš„åˆ†æç»“æœ..."
        )
    
    async def _handle_conversation(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        å¤„ç†å¯¹è¯ - å·²å¼ƒç”¨ï¼Œç»Ÿä¸€ä½¿ç”¨ _handle_react
        """
        # ä¿ç•™æ­¤æ–¹æ³•ä»¥å…¼å®¹ï¼Œä½†å®é™…ä¸Šå·²ä¸å†ä½¿ç”¨
        tool_info = self._get_tool_info_for_react()
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š

{tool_info}

{context}

ç”¨æˆ·: {message}

## å›å¤è§„åˆ™

1. **å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·**ï¼Œè¯·ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰ï¼š
   <tool_call>
   {{"tool": "å·¥å…·å", "args": {{"å‚æ•°å": "å‚æ•°å€¼"}}}}
   </tool_call>

2. **å¦‚æœä¸éœ€è¦å·¥å…·**ï¼Œç›´æ¥å›å¤ç”¨æˆ·ã€‚

3. **å…³äºæ–‡ä»¶æ“ä½œ**ï¼š
   - ç”¨æˆ·æåˆ°è·¯å¾„æ—¶ï¼Œç›´æ¥ä½¿ç”¨ list_directory æˆ– file_read_enhanced
   - ç”¨æˆ·è¯´"åˆ†æé¡¹ç›®"æ—¶ï¼Œå…ˆç”¨ list_directory æŸ¥çœ‹ç»“æ„
   - ä¸è¦è¯´"æˆ‘æ— æ³•è®¿é—®æ–‡ä»¶ç³»ç»Ÿ"ï¼Œä½ æœ‰å·¥å…·å¯ä»¥è®¿é—®

è¯·å›å¤ï¼š"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            
            if "<tool_call>" in response:
                async for chunk in self._execute_react_tools(response, message, context):
                    yield chunk
            else:
                yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤å¤±è´¥: {str(e)}")
    
    async def _execute_react_tools(
        self,
        llm_response: str,
        original_message: str,
        context: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        æ‰§è¡Œ ReAct æ¨¡å¼ä¸­ LLM è¯·æ±‚çš„å·¥å…·è°ƒç”¨
        """
        import re
        import json
        
        # æå–æ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_pattern = r'<tool_call>\s*(\{.*?\})\s*</tool_call>'
        tool_calls = re.findall(tool_pattern, llm_response, re.DOTALL)
        
        if not tool_calls:
            # æ²¡æœ‰æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›åŸå§‹å“åº”
            yield StreamChunk(type="text", content=llm_response)
            return
        
        results = []
        for call_json in tool_calls:
            try:
                call = json.loads(call_json)
                tool_name = call.get("tool")
                args = call.get("args", {})
                
                if tool_name and tool_name in self.tool_orchestrator.tools:
                    yield StreamChunk(
                        type="tool_call",
                        content=f"ğŸ”§ æ‰§è¡Œ {tool_name}...",
                        metadata={"tool": tool_name, "args": args},
                    )
                    
                    success, output = await self.tool_orchestrator.execute(tool_name, args)
                    
                    results.append({
                        "tool": tool_name,
                        "success": success,
                        "output": str(output)[:2000],  # é™åˆ¶é•¿åº¦
                    })
                    
                    yield StreamChunk(
                        type="tool_result",
                        content=f"{'âœ…' if success else 'âŒ'} {tool_name}",
                        metadata={"tool": tool_name, "success": success},
                    )
            except json.JSONDecodeError:
                continue
        
        if not results:
            yield StreamChunk(type="text", content=llm_response)
            return
        
        # æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
        results_text = "\n\n".join([
            f"### {r['tool']}\n```\n{r['output']}\n```" for r in results
        ])
        
        summary_prompt = f"""
{context}

ç”¨æˆ·é—®é¢˜: {original_message}

å·¥å…·æ‰§è¡Œç»“æœ:
{results_text}

è¯·æ ¹æ®ä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´ã€æœ‰å¸®åŠ©çš„å›å¤ã€‚å¦‚æœæ˜¯åˆ†æé¡¹ç›®ï¼Œè¯·æ€»ç»“é¡¹ç›®çš„ä¸»è¦å†…å®¹å’Œç»“æ„ã€‚"""
        
        try:
            summary = await self.llm.chat_completion(
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.5,
            )
            yield StreamChunk(type="text", content=summary)
        except Exception as e:
            # é™çº§ï¼šç›´æ¥è¿”å›å·¥å…·ç»“æœ
            yield StreamChunk(type="text", content=f"å·¥å…·æ‰§è¡Œç»“æœ:\n{results_text}")
    
    async def _handle_simple_task(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†ç®€å•ä»»åŠ¡"""
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®Œæˆç”¨æˆ·çš„è¯·æ±‚ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _handle_tool_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å·¥å…·ä»»åŠ¡"""
        # é€‰æ‹©å·¥å…·
        yield StreamChunk(type="thinking", content="ğŸ”§ é€‰æ‹©åˆé€‚çš„å·¥å…·...")
        
        # ä¼˜å…ˆä½¿ç”¨æ„å›¾ä¸­å»ºè®®çš„å·¥å…·
        selections = []
        if intent.suggested_tools:
            for tool_name in intent.suggested_tools:
                if tool_name in self.tool_orchestrator.tools:
                    # ä»æ¶ˆæ¯ä¸­æå–å‚æ•°
                    args = self._extract_tool_args(tool_name, message, intent)
                    selections.append(ToolSelection(
                        tool_name=tool_name,
                        confidence=0.9,
                        reason=f"æ„å›¾å»ºè®®ä½¿ç”¨ {tool_name}",
                        arguments=args,
                    ))
        
        # å¦‚æœæ„å›¾æ²¡æœ‰å»ºè®®å·¥å…·ï¼Œåˆ™ä½¿ç”¨å…³é”®è¯åŒ¹é…
        if not selections:
            selections = await self.tool_orchestrator.select_tools(message, max_tools=3)
        
        if not selections:
            # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œé™çº§ä¸ºæ™®é€šå›å¤
            async for chunk in self._handle_simple_task(message, context, user_id):
                yield chunk
            return
        
        # æ‰§è¡Œå·¥å…·
        results = []
        for sel in selections:
            yield StreamChunk(
                type="tool_call",
                content=f"ğŸ”§ æ‰§è¡Œ {sel.tool_name}...",
                metadata={"tool": sel.tool_name, "reason": sel.reason},
            )
            
            success, output = await self.tool_orchestrator.execute(
                sel.tool_name,
                sel.arguments,
            )
            
            results.append({
                "tool": sel.tool_name,
                "success": success,
                "output": output,
            })
            
            yield StreamChunk(
                type="tool_result",
                content=f"{'âœ…' if success else 'âŒ'} {sel.tool_name}: {str(output)[:200]}",
                metadata={"tool": sel.tool_name, "success": success},
            )
        
        # ç”Ÿæˆæœ€ç»ˆå›å¤
        result_text = "\n".join([
            f"- {r['tool']}: {r['output']}" for r in results
        ])
        
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

å·¥å…·æ‰§è¡Œç»“æœ:
{result_text}

è¯·æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´çš„å›å¤ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def _handle_complex_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        session_id: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å¤æ‚ä»»åŠ¡ï¼ˆä½¿ç”¨ Agent Loopï¼‰"""
        yield StreamChunk(
            type="thinking",
            content=f"ğŸ¯ æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ (é¢„è®¡ {intent.estimated_steps} æ­¥)ï¼Œå¯åŠ¨ Agent æ¨¡å¼...",
        )
        
        # åˆ›å»º Agent Loop
        loop = AgentLoop(
            llm_client=self.llm,
            tools=self.tool_orchestrator.tools,
            planner=self.planner,
            max_steps=min(intent.estimated_steps + 5, 15),
        )
        
        # æ‰§è¡Œ
        final_content = ""
        
        async for update in loop.execute(message, intent, {"context": context}):
            if update.type == "thinking":
                yield StreamChunk(type="thinking", content=update.message)
                
            elif update.type == "action":
                tool_name = update.data.get("step", {}).get("tool_name", "") if update.data else ""
                yield StreamChunk(
                    type="tool_call",
                    content=update.message,
                    metadata={"tool": tool_name, "step": update.step},
                )
                
            elif update.type == "result":
                yield StreamChunk(
                    type="tool_result",
                    content=update.message,
                    metadata=update.data,
                )
                
            elif update.type == "progress":
                yield StreamChunk(
                    type="progress",
                    content=update.message,
                    metadata={
                        "step": update.step,
                        "total": update.total_steps,
                    },
                )
                
            elif update.type == "complete":
                final_content = update.message
                yield StreamChunk(type="text", content=update.message)
                
            elif update.type == "error":
                yield StreamChunk(type="error", content=update.message)
    
    def _get_conversation_history(
        self,
        session_id: str,
        max_messages: int = 10,
    ) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": []}
        
        history = self.sessions[session_id].get("history", [])
        return history[-max_messages:]
    
    def _extract_tool_args(self, tool_name: str, message: str, intent: Intent) -> Dict[str, Any]:
        """
        ä»æ¶ˆæ¯å’Œæ„å›¾ä¸­æå–å·¥å…·å‚æ•°
        
        Args:
            tool_name: å·¥å…·åç§°
            message: ç”¨æˆ·æ¶ˆæ¯
            intent: è¯†åˆ«çš„æ„å›¾
        
        Returns:
            å·¥å…·å‚æ•°å­—å…¸
        """
        import re
        args = {}
        
        # æå–è·¯å¾„å‚æ•° - åªåŒ¹é… ASCII è·¯å¾„å­—ç¬¦ï¼ˆä¸åŒ…å«ä¸­æ–‡ï¼‰
        path_match = re.search(r'([/\\][a-zA-Z0-9_\-\.\/\\]+)', message)
        
        if tool_name == "list_directory":
            if path_match:
                args["path"] = path_match.group(1)
            else:
                args["path"] = "."  # é»˜è®¤å½“å‰ç›®å½•
                
        elif tool_name == "file_read_enhanced":
            if path_match:
                args["file_path"] = path_match.group(1)
            # ä»æ„å›¾å®ä½“ä¸­è·å–
            if intent.entities and "file_paths" in intent.entities:
                paths = intent.entities["file_paths"]
                if paths:
                    args["file_path"] = paths[0]
                    
        elif tool_name == "shell_execute":
            # å¯¹äº shell å‘½ä»¤ï¼Œæå–å¼•å·å†…å®¹æˆ–å…³é”®è¯åå†…å®¹
            cmd_match = re.search(r'[`\'\"](.*?)[`\'\"]', message)
            if cmd_match:
                args["command"] = cmd_match.group(1)
                
        elif tool_name == "env_info":
            pass  # æ— éœ€å‚æ•°
            
        elif tool_name == "process_list":
            pass  # æ— éœ€å‚æ•°
        
        return args
    
    def _get_tool_info_for_prompt(self) -> str:
        """è·å–å·¥å…·ä¿¡æ¯ç”¨äºæç¤ºè¯"""
        tool_lines = ["## å¯ç”¨å·¥å…·\n"]
        
        for name, meta in self.tool_orchestrator.metadata.items():
            desc = meta.description[:80] if meta.description else "æ— æè¿°"
            tool_lines.append(f"- **{name}**: {desc}")
        
        tool_lines.append("\nå¦‚æœç”¨æˆ·éœ€è¦ä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ä½ å¯ä»¥å¸®å¿™æ‰§è¡Œã€‚")
        return "\n".join(tool_lines)
    
    def _get_tool_info_for_react(self) -> str:
        """è·å–å·¥å…·ä¿¡æ¯ç”¨äº ReAct æ¨¡å¼ï¼ˆåŒ…å«å‚æ•°è¯´æ˜ï¼‰"""
        tool_lines = ["## å¯ç”¨å·¥å…·\n"]
        
        for name, meta in self.tool_orchestrator.metadata.items():
            desc = meta.description if meta.description else "æ— æè¿°"
            tool_lines.append(f"### {name}")
            tool_lines.append(f"æè¿°: {desc}")
            
            # æ·»åŠ å‚æ•°è¯´æ˜
            if meta.input_schema and "properties" in meta.input_schema:
                params = []
                for param_name, param_info in meta.input_schema["properties"].items():
                    param_type = param_info.get("type", "string")
                    param_desc = param_info.get("description", "")
                    required = param_name in meta.input_schema.get("required", [])
                    params.append(f"  - {param_name} ({param_type}{'*' if required else ''}): {param_desc}")
                if params:
                    tool_lines.append("å‚æ•°:")
                    tool_lines.extend(params)
            tool_lines.append("")
        
        return "\n".join(tool_lines)
    
    async def _save_conversation(
        self,
        session_id: str,
        content: str,
        role: str,
    ) -> None:
        """ä¿å­˜å¯¹è¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": []}
        
        self.sessions[session_id]["history"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
        })
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.sessions[session_id]["history"]) > 50:
            self.sessions[session_id]["history"] = self.sessions[session_id]["history"][-50:]
    
    def clear_session(self, session_id: str) -> None:
        """æ¸…é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]
        logger.info(f"Session {session_id} cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "active_sessions": len(self.sessions),
            "total_tools": len(self.tool_orchestrator.tools),
            "enabled_features": {
                "rag": self.enable_rag,
                "skills": self.enable_skills,
                "memory": self.enable_memory,
                "preferences": self.enable_preferences,
            },
        }


# å…¨å±€å®ä¾‹
_orchestrator: Optional[CursorStyleOrchestrator] = None


def get_orchestrator(
    llm_client=None,
    tools: Optional[List[Callable]] = None,
    **kwargs,
) -> CursorStyleOrchestrator:
    """è·å–ç¼–æ’å™¨å®ä¾‹"""
    global _orchestrator
    if _orchestrator is None:
        if llm_client is None:
            raise ValueError("First call requires llm_client")
        _orchestrator = CursorStyleOrchestrator(llm_client, tools, **kwargs)
    return _orchestrator

