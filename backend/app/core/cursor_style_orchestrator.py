# -*- coding: utf-8 -*-
"""
Cursor é£æ ¼ç¼–æ’å™¨ - Cursor-Style Orchestrator

è¿™æ˜¯ agentic_chatBot çš„æ ¸å¿ƒå¼•æ“ï¼Œæ•´åˆæ‰€æœ‰èƒ½åŠ›ï¼š
1. æ·±åº¦æ„å›¾è¯†åˆ« (IntentRecognizer)
2. è‡ªä¸»æ‰§è¡Œå¾ªç¯ (AgentLoop)
3. æ™ºèƒ½å·¥å…·ç¼–æ’ (ToolOrchestrator)
4. ä¸Šä¸‹æ–‡å·¥ç¨‹ (ContextManager)
5. ç”¨æˆ·åå¥½å­¦ä¹  (UserPreferences)
6. RAG çŸ¥è¯†æ£€ç´¢
7. æŠ€èƒ½ç³»ç»Ÿ (Skills)
8. è®°å¿†ç®¡ç† (Memory)

ç›®æ ‡ï¼šè®© ChatBot èƒ½åŠ›åª²ç¾ Cursorï¼
"""
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger
import asyncio
import traceback

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from .intent_recognizer import (
    IntentRecognizer, Intent, TaskType, RequiredCapability, get_intent_recognizer
)
from .agent_loop import (
    AgentLoop, ProgressUpdate, ExecutionPlan, LoopState
)
from .tool_orchestrator import (
    ToolOrchestrator, ToolSelection, get_tool_orchestrator
)
from .context_manager import (
    ContextManager, ContextSource, build_context
)
from .user_preferences import (
    UserPreferenceManager, get_preference_manager
)
from .planner import AgentPlanner
from .memory import MemoryManager
from .skills import SkillsManager, get_skills_manager

# Workspace è‡ªåŠ¨ç´¢å¼•
from ..rag.workspace_indexer import (
    WorkspaceIndexer, get_workspace_indexer, auto_index_workspace, IndexingStatus
)

# Phase 3 & 4: å¯¹è¯å‹ç¼©å’Œå¢å¼ºå·¥å…·
from .session_compactor import SessionCompactor, CompactionResult, get_session_compactor
from .enhanced_tools import get_enhanced_tools


@dataclass
class ChatResponse:
    """
    èŠå¤©å“åº”
    
    åŒ…å«å®Œæ•´çš„å“åº”ä¿¡æ¯
    """
    content: str
    intent: Optional[Intent] = None
    used_tools: List[str] = field(default_factory=list)
    citations: List[Dict[str, str]] = field(default_factory=list)
    execution_steps: int = 0
    duration_ms: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "intent": self.intent.to_dict() if self.intent else None,
            "used_tools": self.used_tools,
            "citations": self.citations,
            "execution_steps": self.execution_steps,
            "duration_ms": self.duration_ms,
            "metadata": self.metadata,
        }


@dataclass
class StreamChunk:
    """
    æµå¼è¾“å‡ºå—
    """
    type: str  # text, thinking, tool_call, tool_result, progress, complete, error
    content: str
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.type,
            "content": self.content,
            "metadata": self.metadata,
        }


class CursorStyleOrchestrator:
    """
    Cursor é£æ ¼ç¼–æ’å™¨
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ AI èƒ½åŠ›ï¼Œæä¾› Cursor çº§åˆ«çš„ ChatBot ä½“éªŒã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    orchestrator = CursorStyleOrchestrator(llm_client)
    
    # æµå¼å¯¹è¯
    async for chunk in orchestrator.chat_stream("åˆ†æè¿™ä¸ªé¡¹ç›®", session_id):
        if chunk.type == "text":
            print(chunk.content, end="")
        elif chunk.type == "thinking":
            print(f"ğŸ’­ {chunk.content}")
    
    # éæµå¼å¯¹è¯
    response = await orchestrator.chat("ä½ å¥½", session_id)
    print(response.content)
    ```
    """
    
    def __init__(
        self,
        llm_client,
        tools: Optional[List[Callable]] = None,
        enable_rag: bool = True,
        enable_skills: bool = True,
        enable_memory: bool = True,
        enable_preferences: bool = True,
        enable_auto_index: bool = True,
        workspace_path: Optional[str] = None,
        max_context_tokens: int = 8000,
    ):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            tools: å·¥å…·å‡½æ•°åˆ—è¡¨
            enable_rag: æ˜¯å¦å¯ç”¨ RAG
            enable_skills: æ˜¯å¦å¯ç”¨æŠ€èƒ½ç³»ç»Ÿ
            enable_memory: æ˜¯å¦å¯ç”¨è®°å¿†
            enable_preferences: æ˜¯å¦å¯ç”¨ç”¨æˆ·åå¥½å­¦ä¹ 
            enable_auto_index: æ˜¯å¦å¯ç”¨å·¥ä½œåŒºè‡ªåŠ¨ç´¢å¼•
            workspace_path: å·¥ä½œåŒºè·¯å¾„ï¼ˆç”¨äºè‡ªåŠ¨ç´¢å¼•ï¼‰
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ Token æ•°
        """
        self.llm = llm_client
        self.max_context_tokens = max_context_tokens
        
        # åŠŸèƒ½å¼€å…³
        self.enable_rag = enable_rag
        self.enable_skills = enable_skills
        self.enable_memory = enable_memory
        self.enable_preferences = enable_preferences
        self.enable_auto_index = enable_auto_index
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.intent_recognizer = get_intent_recognizer(llm_client)
        self.planner = AgentPlanner(llm_client)
        self.tool_orchestrator = get_tool_orchestrator(llm_client)
        self.preference_manager = get_preference_manager() if enable_preferences else None
        self.memory_manager = MemoryManager() if enable_memory else None
        self.skill_manager = get_skills_manager() if enable_skills else None
        
        # Phase 3: ä¼šè¯å‹ç¼©å™¨
        self.session_compactor = get_session_compactor(llm_client)
        
        # Workspace è‡ªåŠ¨ç´¢å¼•å™¨
        self.workspace_indexer: Optional[WorkspaceIndexer] = None
        self._workspace_path = workspace_path
        self._indexing_task: Optional[asyncio.Task] = None
        
        # æ³¨å†Œå·¥å…·
        if tools:
            self.tool_orchestrator.register_many(tools)
        
        # Phase 4: æ³¨å†Œå¢å¼ºå·¥å…·
        enhanced_tools = get_enhanced_tools()
        self.tool_orchestrator.register_many(enhanced_tools)
        
        # ä¼šè¯çŠ¶æ€
        self.sessions: Dict[str, Dict[str, Any]] = {}
        
        logger.info(
            f"CursorStyleOrchestrator initialized: "
            f"RAG={enable_rag}, Skills={enable_skills}, "
            f"Memory={enable_memory}, Preferences={enable_preferences}, "
            f"AutoIndex={enable_auto_index}"
        )
    
    async def initialize_workspace_index(
        self,
        workspace_path: Optional[str] = None,
        priority_only: bool = True,
        background: bool = True,
    ) -> Optional[IndexingStatus]:
        """
        åˆå§‹åŒ–å·¥ä½œåŒºç´¢å¼•
        
        Args:
            workspace_path: å·¥ä½œåŒºè·¯å¾„ï¼Œä¸æä¾›åˆ™ä½¿ç”¨æ„é€ å‡½æ•°ä¸­çš„è·¯å¾„
            priority_only: æ˜¯å¦åªç´¢å¼•é«˜ä¼˜å…ˆçº§æ–‡ä»¶ï¼ˆæ¨èé¦–æ¬¡ä½¿ç”¨ï¼‰
            background: æ˜¯å¦åœ¨åå°è¿è¡Œ
        
        Returns:
            å¦‚æœéåå°è¿è¡Œï¼Œè¿”å›ç´¢å¼•çŠ¶æ€ï¼›åå°è¿è¡Œè¿”å› None
        """
        if not self.enable_auto_index:
            logger.info("Auto indexing is disabled")
            return None
        
        path = workspace_path or self._workspace_path
        if not path:
            logger.warning("No workspace path provided for indexing")
            return None
        
        try:
            # åˆå§‹åŒ–ç´¢å¼•å™¨
            self.workspace_indexer = get_workspace_indexer(path)
            
            if background:
                # åå°ç´¢å¼•
                self._indexing_task = asyncio.create_task(
                    self._background_index(priority_only)
                )
                logger.info(f"Started background indexing for: {path}")
                return None
            else:
                # åŒæ­¥ç´¢å¼•
                status = await self.workspace_indexer.index_workspace(
                    priority_only=priority_only
                )
                logger.info(f"Workspace indexing complete: {status.indexed_files} files indexed")
                return status
                
        except Exception as e:
            logger.error(f"Failed to initialize workspace index: {e}")
            return None
    
    async def _background_index(self, priority_only: bool = True):
        """åå°ç´¢å¼•ä»»åŠ¡"""
        try:
            if self.workspace_indexer:
                # ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿç´¢å¼•é«˜ä¼˜å…ˆçº§æ–‡ä»¶
                await self.workspace_indexer.index_workspace(priority_only=True)
                logger.info("Phase 1: Priority files indexed")
                
                # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœä¸æ˜¯åªç´¢å¼•é«˜ä¼˜å…ˆçº§ï¼Œåˆ™ç»§ç»­ç´¢å¼•å…¶ä»–æ–‡ä»¶
                if not priority_only:
                    await asyncio.sleep(5)  # ç¨ç­‰ä¸€ä¸‹ï¼Œé¿å…å½±å“ç”¨æˆ·ä½“éªŒ
                    await self.workspace_indexer.index_workspace(priority_only=False)
                    logger.info("Phase 2: All files indexed")
                    
        except asyncio.CancelledError:
            logger.info("Background indexing cancelled")
        except Exception as e:
            logger.error(f"Background indexing failed: {e}")
    
    def get_indexing_status(self) -> Optional[IndexingStatus]:
        """è·å–ç´¢å¼•çŠ¶æ€"""
        if self.workspace_indexer:
            return self.workspace_indexer.get_status()
        return None
    
    def get_indexed_files(self) -> List[str]:
        """è·å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
        if self.workspace_indexer:
            return self.workspace_indexer.get_indexed_files()
        return []
    
    async def chat_stream(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼å¯¹è¯æ¥å£ - çº¯ ReAct æ¨¡å¼
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰
            files: å¼•ç”¨çš„æ–‡ä»¶ {path: content}
            rag_query: RAG æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ messageï¼‰
        
        Yields:
            StreamChunk æµå¼è¾“å‡ºå—
        """
        start_time = datetime.now()
        user_id = user_id or session_id
        used_tools = []
        
        try:
            # ==================== 1. æ„å»ºä¸Šä¸‹æ–‡ ====================
            yield StreamChunk(type="thinking", content="ğŸ“š æ”¶é›†ç›¸å…³ä¿¡æ¯...")
            
            # ç®€å•æ„å›¾è¯†åˆ«ï¼ˆä»…ç”¨äºä¸Šä¸‹æ–‡æ„å»ºï¼Œä¸ç”¨äºè·¯ç”±ï¼‰
            intent = await self.intent_recognizer.recognize(
                message,
                history=self._get_conversation_history(session_id),
                available_tools=list(self.tool_orchestrator.tools.keys()),
            )
            
            task_type_str = intent.task_type.value if hasattr(intent.task_type, 'value') else str(intent.task_type)
            logger.info(f"Intent (for context): {task_type_str}")
            
            context = await self._build_context(
                message=message,
                session_id=session_id,
                user_id=user_id,
                intent=intent,
                files=files,
                rag_query=rag_query,
            )
            
            # ==================== 2. çº¯ ReAct æ¨¡å¼å¤„ç† ====================
            
            assistant_response_parts = []
            
            async for chunk in self._handle_react(message, context, session_id, user_id):
                if chunk.type == "text":
                    assistant_response_parts.append(chunk.content or "")
                yield chunk
                if chunk.metadata and chunk.metadata.get("tool"):
                    used_tools.append(chunk.metadata["tool"])
            
            # ==================== 4. å­¦ä¹ å’Œè®°å½• ====================
            if self.enable_preferences:
                self.preference_manager.learn_from_message(user_id, message)
                for tool in used_tools:
                    self.preference_manager.learn_from_tool_usage(user_id, tool, True)
            
            # è®°å½•å¯¹è¯ - ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤
            await self._save_conversation(session_id, message, "user")
            assistant_response = "".join(assistant_response_parts)
            if assistant_response:
                await self._save_conversation(session_id, assistant_response, "assistant")
            
            # è®¡ç®—è€—æ—¶
            duration = (datetime.now() - start_time).total_seconds() * 1000
            
            yield StreamChunk(
                type="complete",
                content="",
                metadata={
                    "duration_ms": int(duration),
                    "used_tools": used_tools,
                    "intent": intent.to_dict(),
                },
            )
            
        except Exception as e:
            logger.error(f"Chat error: {e}\n{traceback.format_exc()}")
            yield StreamChunk(
                type="error",
                content=f"âŒ å¤„ç†å¤±è´¥: {str(e)}",
            )
    
    async def chat(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
    ) -> ChatResponse:
        """
        éæµå¼å¯¹è¯æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· ID
            files: å¼•ç”¨çš„æ–‡ä»¶
        
        Returns:
            ChatResponse å®Œæ•´å“åº”
        """
        content_parts = []
        used_tools = []
        intent = None
        duration = 0
        
        async for chunk in self.chat_stream(message, session_id, user_id, files):
            if chunk.type == "text":
                content_parts.append(chunk.content)
            elif chunk.type in ["tool_call", "tool_result"]:
                if chunk.metadata and chunk.metadata.get("tool"):
                    used_tools.append(chunk.metadata["tool"])
            elif chunk.type == "complete" and chunk.metadata:
                intent_data = chunk.metadata.get("intent")
                if intent_data:
                    intent = self._intent_from_dict(intent_data) if isinstance(intent_data, dict) else intent_data
                duration = chunk.metadata.get("duration_ms", 0)
        
        return ChatResponse(
            content="".join(content_parts),
            intent=intent,
            used_tools=list(set(used_tools)),
            duration_ms=duration,
        )
    
    def _intent_from_dict(self, data: Dict[str, Any]) -> Intent:
        """ä»å­—å…¸é‡å»º Intentï¼Œæ­£ç¡®å¤„ç†æšä¸¾ç±»å‹è½¬æ¢"""
        from .intent_recognizer import TaskType, RequiredCapability
        
        # å¤„ç† task_type: å­—ç¬¦ä¸² -> æšä¸¾
        task_type = data.get("task_type", "conversation")
        if isinstance(task_type, str):
            try:
                task_type = TaskType(task_type)
            except ValueError:
                task_type = TaskType.CONVERSATION
        
        # å¤„ç† required_capabilities: å­—ç¬¦ä¸²åˆ—è¡¨ -> æšä¸¾åˆ—è¡¨
        capabilities = data.get("required_capabilities", [])
        if capabilities and isinstance(capabilities[0], str):
            valid_values = [e.value for e in RequiredCapability]
            capabilities = [
                RequiredCapability(c) for c in capabilities 
                if c in valid_values
            ]
        
        return Intent(
            surface_intent=data.get("surface_intent", ""),
            deep_intent=data.get("deep_intent", ""),
            task_type=task_type,
            required_capabilities=capabilities,
            suggested_tools=data.get("suggested_tools", []),
            complexity=data.get("complexity", "low"),
            is_multi_step=data.get("is_multi_step", False),
            estimated_steps=data.get("estimated_steps", 1),
            references_history=data.get("references_history", False),
            context_keywords=data.get("context_keywords", []),
            expected_output_format=data.get("expected_output_format", "text"),
            response_style=data.get("response_style", "detailed"),
            confidence=data.get("confidence", 0.8),
            entities=data.get("entities", {}),
            metadata=data.get("metadata", {}),
        )
    
    async def _build_context(
        self,
        message: str,
        session_id: str,
        user_id: str,
        intent: Intent,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> str:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        cm = ContextManager(max_tokens=self.max_context_tokens)
        
        # 1. ç”¨æˆ·åå¥½ï¼ˆé£æ ¼æç¤ºï¼‰- ä»…å¯¹éç®€å•å¯¹è¯åº”ç”¨
        # å¯¹äºç®€å•å¯¹è¯ï¼ˆå¦‚é—®å€™ï¼‰ï¼Œè·³è¿‡è¯¦ç»†åå¥½ä»¥åŠ å¿«å“åº”
        if self.enable_preferences and intent.task_type != TaskType.CONVERSATION:
            style_prompt = self.preference_manager.get_style_prompt(user_id)
            if style_prompt:
                cm.add(
                    content=style_prompt,
                    source=ContextSource.SYSTEM,
                    title="ç”¨æˆ·åå¥½",
                )
        
        # 2. æŠ€èƒ½æŒ‡ä»¤
        if self.enable_skills and RequiredCapability.SKILLS in intent.required_capabilities:
            relevant_skills = self.skill_manager.match_skills(message)
            for skill in relevant_skills[:2]:
                # Convert examples from Dict to formatted strings
                examples_str = []
                if skill.examples:
                    for ex in skill.examples:
                        if isinstance(ex, dict):
                            examples_str.append(f"ç”¨æˆ·: {ex.get('user', '')}\nåŠ©æ‰‹: {ex.get('assistant', '')}")
                        else:
                            examples_str.append(str(ex))
                cm.add_skill_instructions(
                    skill.name,
                    skill.instructions,
                    examples_str if examples_str else None,
                )
        
        # 3. RAG æ£€ç´¢
        if self.enable_rag and RequiredCapability.RAG in intent.required_capabilities:
            try:
                from ..rag import retriever
                rag_results = await retriever.retrieve(
                    query=rag_query or message,
                    top_k=5,
                )
                cm.add_rag_results(rag_results)
            except Exception as e:
                logger.warning(f"RAG retrieval failed: {e}")
        
        # 4. æ–‡ä»¶å†…å®¹
        if files:
            for path, content in files.items():
                cm.add_file_content(path, content)
        
        # 5. é•¿æœŸè®°å¿†
        if self.enable_memory and RequiredCapability.MEMORY in intent.required_capabilities:
            try:
                memories = await self.memory_manager.get_relevant_long_term_memory(
                    session_id, message
                )
                cm.add_memory(memories)
            except Exception as e:
                logger.warning(f"Memory retrieval failed: {e}")
        
        # 6. å¯¹è¯å†å²
        history = self._get_conversation_history(session_id, include_tool_results=False)
        if history:
            cm.add_conversation_history(history, max_messages=5)
        
        # 7. ğŸ†• ä¹‹å‰çš„å·¥å…·è°ƒç”¨ç»“æœï¼ˆé‡è¦ï¼šè®© AI è®°ä½ä¹‹å‰çš„æ“ä½œï¼‰
        tool_context = self.get_session_context_summary(session_id)
        if tool_context:
            cm.add(
                content=tool_context,
                source=ContextSource.SYSTEM,
                title="ä¹‹å‰çš„å·¥å…·è°ƒç”¨ç»“æœ",
            )
            logger.info(f"Added tool context to session {session_id}: {len(tool_context)} chars")
        else:
            logger.debug(f"No tool context for session {session_id}")
        
        return cm.build()
    
    async def _handle_react(
        self,
        message: str,
        context: str,
        session_id: str,
        user_id: str,
        max_iterations: int = 10,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        çº¯ ReAct æ¨¡å¼å¤„ç†
        
        ReAct å¾ªç¯:
        1. Thought: LLM åˆ†æé—®é¢˜ï¼Œå†³å®šä¸‹ä¸€æ­¥
        2. Action: å¦‚æœéœ€è¦ï¼Œè°ƒç”¨å·¥å…·
        3. Observation: è·å–å·¥å…·ç»“æœ
        4. é‡å¤ç›´åˆ° LLM ç»™å‡º Final Answer
        """
        import re
        import json
        
        tool_info = self._get_tool_info_for_react()
        
        # ReAct ç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œä½¿ç”¨ ReActï¼ˆReasoning + Actingï¼‰æ¨¡å¼æ¥å¸®åŠ©ç”¨æˆ·ã€‚

## å¯ç”¨å·¥å…·

{tool_info}

## ReAct æ ¼å¼

æ¯æ¬¡å›å¤è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

**å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼š**
```
Thought: [åˆ†æå½“å‰æƒ…å†µï¼Œæ€è€ƒä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆ]
Action: <tool_call>{{"tool": "å·¥å…·å", "args": {{"å‚æ•°": "å€¼"}}}}</tool_call>
```

**å¦‚æœå¯ä»¥ç›´æ¥å›ç­”ï¼ˆä¸éœ€è¦å·¥å…·æˆ–å·²è·å¾—è¶³å¤Ÿä¿¡æ¯ï¼‰ï¼š**
```
Thought: [æ€»ç»“æ€è€ƒè¿‡ç¨‹]
Final Answer: [ç»™ç”¨æˆ·çš„å®Œæ•´å›å¤]
```

## é‡è¦è§„åˆ™

1. **ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¸­å·²æœ‰ç›¸å…³ä¿¡æ¯ï¼ˆå¦‚ä¹‹å‰çš„å·¥å…·è°ƒç”¨ç»“æœï¼‰ï¼Œç›´æ¥ä½¿ç”¨
2. **æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ª Action**ï¼Œç­‰å¾… Observation åå†å†³å®šä¸‹ä¸€æ­¥
3. **3-5 æ¬¡å·¥å…·è°ƒç”¨ååº”ç»™å‡ºç­”æ¡ˆ**ï¼šæ¢ç´¢ 3-5 æ¬¡åï¼ŒåŸºäºå·²æ”¶é›†çš„ä¿¡æ¯ç»™å‡º Final Answer
4. **ä¸è¦è¿‡åº¦æ¢ç´¢**ï¼šäº†è§£é¡¹ç›®æ¶æ„åªéœ€æŸ¥çœ‹å…³é”®ç›®å½•å’Œæ–‡ä»¶ï¼Œä¸éœ€è¦éå†æ‰€æœ‰å­ç›®å½•
5. **æ–‡ä»¶/ç›®å½•æ“ä½œ**ï¼šä½¿ç”¨ list_directory æˆ– file_read_enhanced
6. **æ°¸è¿œä¸è¦è¯´"æˆ‘æ— æ³•è®¿é—®"**ï¼Œä½ æœ‰å·¥å…·å¯ä»¥è®¿é—®æ–‡ä»¶ç³»ç»Ÿ
7. **ç®€å•å¯¹è¯**ï¼ˆå¦‚é—®å€™ã€æ„Ÿè°¢ï¼‰å¯ä»¥ç›´æ¥ç»™å‡º Final Answer
8. **ä¿¡æ¯å……è¶³æ—¶ç›´æ¥å›ç­”**ï¼šæœ‰è¶³å¤Ÿä¿¡æ¯å°±ç»™å‡º Final Answerï¼Œä¸è¦è¿½æ±‚å®Œç¾

## ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«ä¹‹å‰çš„å¯¹è¯å’Œå·¥å…·è°ƒç”¨ç»“æœï¼‰

{context}

**æ³¨æ„**ï¼šä¸Šä¸‹æ–‡ä¸­å¯èƒ½åŒ…å«ä¹‹å‰å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œè¯·ä¼˜å…ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œé¿å…é‡å¤è°ƒç”¨ç›¸åŒçš„å·¥å…·ã€‚
"""
        
        # å¯¹è¯å†å²ï¼ˆç”¨äº ReAct å¾ªç¯ï¼‰
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message},
        ]
        
        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            
            yield StreamChunk(
                type="thinking", 
                content=f"ğŸ§  æ€è€ƒä¸­... (æ­¥éª¤ {iteration}/{max_iterations})"
            )
            
            try:
                response = await self.llm.chat_completion(
                    messages=messages,
                    temperature=0.3,
                )
            except Exception as e:
                yield StreamChunk(type="error", content=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ Final Answer
            final_match = re.search(r'Final Answer:\s*(.+)', response, re.DOTALL)
            if final_match:
                final_answer = final_match.group(1).strip()
                yield StreamChunk(type="text", content=final_answer)
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_match = re.search(r'<tool_call>\s*(\{.*?\})\s*</tool_call>', response, re.DOTALL)
            if tool_match:
                try:
                    call = json.loads(tool_match.group(1))
                    tool_name = call.get("tool")
                    args = call.get("args", {})
                    
                    if tool_name and tool_name in self.tool_orchestrator.tools:
                        yield StreamChunk(
                            type="tool_call",
                            content=f"ğŸ”§ æ‰§è¡Œ {tool_name}...",
                            metadata={"tool": tool_name, "args": args},
                        )
                        
                        success, output = await self.tool_orchestrator.execute(tool_name, args)
                        
                        # é™åˆ¶è¾“å‡ºé•¿åº¦
                        output_str = str(output)[:3000]
                        
                        yield StreamChunk(
                            type="tool_result",
                            content=f"{'âœ…' if success else 'âŒ'} {tool_name}",
                            metadata={"tool": tool_name, "success": success},
                        )
                        
                        # ğŸ†• ä¿å­˜å·¥å…·ç»“æœåˆ°ä¼šè¯è®°å¿†
                        await self._save_tool_result(
                            session_id=session_id,
                            tool_name=tool_name,
                            args=args,
                            result=output_str,
                            success=success,
                        )
                        
                        # å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œç»§ç»­ ReAct å¾ªç¯
                        messages.append({"role": "assistant", "content": response})
                        messages.append({
                            "role": "user", 
                            "content": f"Observation: {output_str}\n\nè¯·ç»§ç»­åˆ†æå¹¶å†³å®šä¸‹ä¸€æ­¥ã€‚å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç»™å‡º Final Answerã€‚"
                        })
                        continue
                    else:
                        # å·¥å…·ä¸å­˜åœ¨
                        messages.append({"role": "assistant", "content": response})
                        messages.append({
                            "role": "user",
                            "content": f"Observation: é”™è¯¯ - å·¥å…· '{tool_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥å…·: {list(self.tool_orchestrator.tools.keys())}"
                        })
                        continue
                        
                except json.JSONDecodeError as e:
                    messages.append({"role": "assistant", "content": response})
                    messages.append({
                        "role": "user",
                        "content": f"Observation: å·¥å…·è°ƒç”¨æ ¼å¼é”™è¯¯: {str(e)}ã€‚è¯·ä½¿ç”¨æ­£ç¡®çš„ JSON æ ¼å¼ã€‚"
                    })
                    continue
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¹Ÿæ²¡æœ‰ Final Answerï¼Œå¯èƒ½æ˜¯ç›´æ¥å›å¤
            # å°è¯•æå– Thought åçš„å†…å®¹ä½œä¸ºå›å¤
            thought_match = re.search(r'Thought:\s*(.+?)(?=Action:|Final Answer:|$)', response, re.DOTALL)
            if thought_match and not tool_match:
                # LLM å¯èƒ½å¿˜è®°äº† Final Answer æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨å›å¤
                yield StreamChunk(type="text", content=response)
                return
            
            # å…¶ä»–æƒ…å†µï¼Œç›´æ¥è¿”å› LLM çš„å›å¤
            yield StreamChunk(type="text", content=response)
            return
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        yield StreamChunk(
            type="text", 
            content="æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘å°è¯•äº†å¤šæ¬¡ä½†æœªèƒ½å®Œå…¨è§£å†³ã€‚ä»¥ä¸‹æ˜¯æˆ‘ç›®å‰çš„åˆ†æç»“æœ..."
        )
    
    async def _handle_conversation(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        å¤„ç†å¯¹è¯ - å·²å¼ƒç”¨ï¼Œç»Ÿä¸€ä½¿ç”¨ _handle_react
        """
        # ä¿ç•™æ­¤æ–¹æ³•ä»¥å…¼å®¹ï¼Œä½†å®é™…ä¸Šå·²ä¸å†ä½¿ç”¨
        tool_info = self._get_tool_info_for_react()
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š

{tool_info}

{context}

ç”¨æˆ·: {message}

## å›å¤è§„åˆ™

1. **å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·**ï¼Œè¯·ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰ï¼š
   <tool_call>
   {{"tool": "å·¥å…·å", "args": {{"å‚æ•°å": "å‚æ•°å€¼"}}}}
   </tool_call>

2. **å¦‚æœä¸éœ€è¦å·¥å…·**ï¼Œç›´æ¥å›å¤ç”¨æˆ·ã€‚

3. **å…³äºæ–‡ä»¶æ“ä½œ**ï¼š
   - ç”¨æˆ·æåˆ°è·¯å¾„æ—¶ï¼Œç›´æ¥ä½¿ç”¨ list_directory æˆ– file_read_enhanced
   - ç”¨æˆ·è¯´"åˆ†æé¡¹ç›®"æ—¶ï¼Œå…ˆç”¨ list_directory æŸ¥çœ‹ç»“æ„
   - ä¸è¦è¯´"æˆ‘æ— æ³•è®¿é—®æ–‡ä»¶ç³»ç»Ÿ"ï¼Œä½ æœ‰å·¥å…·å¯ä»¥è®¿é—®

è¯·å›å¤ï¼š"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            
            if "<tool_call>" in response:
                async for chunk in self._execute_react_tools(response, message, context):
                    yield chunk
            else:
                yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤å¤±è´¥: {str(e)}")
    
    async def _execute_react_tools(
        self,
        llm_response: str,
        original_message: str,
        context: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        æ‰§è¡Œ ReAct æ¨¡å¼ä¸­ LLM è¯·æ±‚çš„å·¥å…·è°ƒç”¨
        """
        import re
        import json
        
        # æå–æ‰€æœ‰å·¥å…·è°ƒç”¨
        tool_pattern = r'<tool_call>\s*(\{.*?\})\s*</tool_call>'
        tool_calls = re.findall(tool_pattern, llm_response, re.DOTALL)
        
        if not tool_calls:
            # æ²¡æœ‰æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›åŸå§‹å“åº”
            yield StreamChunk(type="text", content=llm_response)
            return
        
        results = []
        for call_json in tool_calls:
            try:
                call = json.loads(call_json)
                tool_name = call.get("tool")
                args = call.get("args", {})
                
                if tool_name and tool_name in self.tool_orchestrator.tools:
                    yield StreamChunk(
                        type="tool_call",
                        content=f"ğŸ”§ æ‰§è¡Œ {tool_name}...",
                        metadata={"tool": tool_name, "args": args},
                    )
                    
                    success, output = await self.tool_orchestrator.execute(tool_name, args)
                    
                    results.append({
                        "tool": tool_name,
                        "success": success,
                        "output": str(output)[:2000],  # é™åˆ¶é•¿åº¦
                    })
                    
                    yield StreamChunk(
                        type="tool_result",
                        content=f"{'âœ…' if success else 'âŒ'} {tool_name}",
                        metadata={"tool": tool_name, "success": success},
                    )
            except json.JSONDecodeError:
                continue
        
        if not results:
            yield StreamChunk(type="text", content=llm_response)
            return
        
        # æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
        results_text = "\n\n".join([
            f"### {r['tool']}\n```\n{r['output']}\n```" for r in results
        ])
        
        summary_prompt = f"""
{context}

ç”¨æˆ·é—®é¢˜: {original_message}

å·¥å…·æ‰§è¡Œç»“æœ:
{results_text}

è¯·æ ¹æ®ä»¥ä¸Šå·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´ã€æœ‰å¸®åŠ©çš„å›å¤ã€‚å¦‚æœæ˜¯åˆ†æé¡¹ç›®ï¼Œè¯·æ€»ç»“é¡¹ç›®çš„ä¸»è¦å†…å®¹å’Œç»“æ„ã€‚"""
        
        try:
            summary = await self.llm.chat_completion(
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.5,
            )
            yield StreamChunk(type="text", content=summary)
        except Exception as e:
            # é™çº§ï¼šç›´æ¥è¿”å›å·¥å…·ç»“æœ
            yield StreamChunk(type="text", content=f"å·¥å…·æ‰§è¡Œç»“æœ:\n{results_text}")
    
    async def _handle_simple_task(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†ç®€å•ä»»åŠ¡"""
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®Œæˆç”¨æˆ·çš„è¯·æ±‚ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _handle_tool_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å·¥å…·ä»»åŠ¡"""
        # é€‰æ‹©å·¥å…·
        yield StreamChunk(type="thinking", content="ğŸ”§ é€‰æ‹©åˆé€‚çš„å·¥å…·...")
        
        # ä¼˜å…ˆä½¿ç”¨æ„å›¾ä¸­å»ºè®®çš„å·¥å…·
        selections = []
        if intent.suggested_tools:
            for tool_name in intent.suggested_tools:
                if tool_name in self.tool_orchestrator.tools:
                    # ä»æ¶ˆæ¯ä¸­æå–å‚æ•°
                    args = self._extract_tool_args(tool_name, message, intent)
                    selections.append(ToolSelection(
                        tool_name=tool_name,
                        confidence=0.9,
                        reason=f"æ„å›¾å»ºè®®ä½¿ç”¨ {tool_name}",
                        arguments=args,
                    ))
        
        # å¦‚æœæ„å›¾æ²¡æœ‰å»ºè®®å·¥å…·ï¼Œåˆ™ä½¿ç”¨å…³é”®è¯åŒ¹é…
        if not selections:
            selections = await self.tool_orchestrator.select_tools(message, max_tools=3)
        
        if not selections:
            # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œé™çº§ä¸ºæ™®é€šå›å¤
            async for chunk in self._handle_simple_task(message, context, user_id):
                yield chunk
            return
        
        # æ‰§è¡Œå·¥å…·
        results = []
        for sel in selections:
            yield StreamChunk(
                type="tool_call",
                content=f"ğŸ”§ æ‰§è¡Œ {sel.tool_name}...",
                metadata={"tool": sel.tool_name, "reason": sel.reason},
            )
            
            success, output = await self.tool_orchestrator.execute(
                sel.tool_name,
                sel.arguments,
            )
            
            results.append({
                "tool": sel.tool_name,
                "success": success,
                "output": output,
            })
            
            yield StreamChunk(
                type="tool_result",
                content=f"{'âœ…' if success else 'âŒ'} {sel.tool_name}: {str(output)[:200]}",
                metadata={"tool": sel.tool_name, "success": success},
            )
        
        # ç”Ÿæˆæœ€ç»ˆå›å¤
        result_text = "\n".join([
            f"- {r['tool']}: {r['output']}" for r in results
        ])
        
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

å·¥å…·æ‰§è¡Œç»“æœ:
{result_text}

è¯·æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´çš„å›å¤ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def _handle_complex_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        session_id: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å¤æ‚ä»»åŠ¡ï¼ˆä½¿ç”¨ Agent Loopï¼‰"""
        yield StreamChunk(
            type="thinking",
            content=f"ğŸ¯ æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ (é¢„è®¡ {intent.estimated_steps} æ­¥)ï¼Œå¯åŠ¨ Agent æ¨¡å¼...",
        )
        
        # åˆ›å»º Agent Loop
        loop = AgentLoop(
            llm_client=self.llm,
            tools=self.tool_orchestrator.tools,
            planner=self.planner,
            max_steps=min(intent.estimated_steps + 5, 15),
        )
        
        # æ‰§è¡Œ
        final_content = ""
        
        async for update in loop.execute(message, intent, {"context": context}):
            if update.type == "thinking":
                yield StreamChunk(type="thinking", content=update.message)
                
            elif update.type == "action":
                tool_name = update.data.get("step", {}).get("tool_name", "") if update.data else ""
                yield StreamChunk(
                    type="tool_call",
                    content=update.message,
                    metadata={"tool": tool_name, "step": update.step},
                )
                
            elif update.type == "result":
                yield StreamChunk(
                    type="tool_result",
                    content=update.message,
                    metadata=update.data,
                )
                
            elif update.type == "progress":
                yield StreamChunk(
                    type="progress",
                    content=update.message,
                    metadata={
                        "step": update.step,
                        "total": update.total_steps,
                    },
                )
                
            elif update.type == "complete":
                final_content = update.message
                yield StreamChunk(type="text", content=update.message)
                
            elif update.type == "error":
                yield StreamChunk(type="error", content=update.message)
    
    def _get_conversation_history(
        self,
        session_id: str,
        max_messages: int = 10,
        include_tool_results: bool = True,
    ) -> List[Dict[str, str]]:
        """
        è·å–å¯¹è¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
            max_messages: æœ€å¤§æ¶ˆæ¯æ•°
            include_tool_results: æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ç»“æœ
        
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å†å²åˆ—è¡¨
        """
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": [], "tool_results": []}
        
        history = self.sessions[session_id].get("history", [])
        
        # æ ¼å¼åŒ–å†å²è®°å½•
        formatted_history = []
        for item in history[-max_messages:]:
            role = item.get("role", "user")
            content = item.get("content", "")
            
            if role == "tool" and include_tool_results:
                # å·¥å…·ç»“æœä½œä¸ºç³»ç»Ÿä¿¡æ¯
                formatted_history.append({
                    "role": "system",
                    "content": f"[ä¹‹å‰çš„å·¥å…·è°ƒç”¨ç»“æœ]\n{content}",
                })
            elif role in ["user", "assistant"]:
                formatted_history.append({
                    "role": role,
                    "content": content,
                })
        
        return formatted_history
    
    def get_session_context_summary(
        self,
        session_id: str,
    ) -> str:
        """
        è·å–ä¼šè¯ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆç”¨äºæä¾›ç»™ LLM ä½œä¸ºèƒŒæ™¯çŸ¥è¯†ï¼‰
        
        Returns:
            åŒ…å«æœ€è¿‘å·¥å…·è°ƒç”¨ç»“æœçš„æ‘˜è¦æ–‡æœ¬
        """
        if session_id not in self.sessions:
            return ""
        
        tool_results = self.sessions[session_id].get("tool_results", [])
        if not tool_results:
            return ""
        
        # ç”Ÿæˆæ‘˜è¦
        summary_parts = ["## ä¹‹å‰çš„æ“ä½œç»“æœï¼ˆä½ å·²ç»è·å–çš„ä¿¡æ¯ï¼Œè¯·ç›´æ¥ä½¿ç”¨ï¼Œä¸è¦é‡å¤è°ƒç”¨å·¥å…·ï¼‰\n"]
        
        for result in tool_results[-5:]:  # æœ€è¿‘ 5 ä¸ª
            tool_name = result.get("tool", "unknown")
            success = result.get("success", False)
            args = result.get("args", {})
            content = result.get("result", "")[:2000]  # å¢åŠ åˆ° 2000 å­—ç¬¦
            
            status = "âœ…" if success else "âŒ"
            args_str = ", ".join(f"{k}={v}" for k, v in args.items()) if args else ""
            summary_parts.append(f"### {status} {tool_name}({args_str})")
            summary_parts.append(f"```\n{content}\n```\n")
        
        return "\n".join(summary_parts)
    
    def _extract_tool_args(self, tool_name: str, message: str, intent: Intent) -> Dict[str, Any]:
        """
        ä»æ¶ˆæ¯å’Œæ„å›¾ä¸­æå–å·¥å…·å‚æ•°
        
        Args:
            tool_name: å·¥å…·åç§°
            message: ç”¨æˆ·æ¶ˆæ¯
            intent: è¯†åˆ«çš„æ„å›¾
        
        Returns:
            å·¥å…·å‚æ•°å­—å…¸
        """
        import re
        args = {}
        
        # æå–è·¯å¾„å‚æ•° - åªåŒ¹é… ASCII è·¯å¾„å­—ç¬¦ï¼ˆä¸åŒ…å«ä¸­æ–‡ï¼‰
        path_match = re.search(r'([/\\][a-zA-Z0-9_\-\.\/\\]+)', message)
        
        if tool_name == "list_directory":
            if path_match:
                args["path"] = path_match.group(1)
            else:
                args["path"] = "."  # é»˜è®¤å½“å‰ç›®å½•
                
        elif tool_name == "file_read_enhanced":
            if path_match:
                args["file_path"] = path_match.group(1)
            # ä»æ„å›¾å®ä½“ä¸­è·å–
            if intent.entities and "file_paths" in intent.entities:
                paths = intent.entities["file_paths"]
                if paths:
                    args["file_path"] = paths[0]
                    
        elif tool_name == "shell_execute":
            # å¯¹äº shell å‘½ä»¤ï¼Œæå–å¼•å·å†…å®¹æˆ–å…³é”®è¯åå†…å®¹
            cmd_match = re.search(r'[`\'\"](.*?)[`\'\"]', message)
            if cmd_match:
                args["command"] = cmd_match.group(1)
                
        elif tool_name == "env_info":
            pass  # æ— éœ€å‚æ•°
            
        elif tool_name == "process_list":
            pass  # æ— éœ€å‚æ•°
        
        return args
    
    def _get_tool_info_for_prompt(self) -> str:
        """è·å–å·¥å…·ä¿¡æ¯ç”¨äºæç¤ºè¯"""
        tool_lines = ["## å¯ç”¨å·¥å…·\n"]
        
        for name, meta in self.tool_orchestrator.metadata.items():
            desc = meta.description[:80] if meta.description else "æ— æè¿°"
            tool_lines.append(f"- **{name}**: {desc}")
        
        tool_lines.append("\nå¦‚æœç”¨æˆ·éœ€è¦ä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ä½ å¯ä»¥å¸®å¿™æ‰§è¡Œã€‚")
        return "\n".join(tool_lines)
    
    def _get_tool_info_for_react(self) -> str:
        """è·å–å·¥å…·ä¿¡æ¯ç”¨äº ReAct æ¨¡å¼ï¼ˆåŒ…å«å‚æ•°è¯´æ˜ï¼‰"""
        tool_lines = ["## å¯ç”¨å·¥å…·\n"]
        
        for name, meta in self.tool_orchestrator.metadata.items():
            desc = meta.description if meta.description else "æ— æè¿°"
            tool_lines.append(f"### {name}")
            tool_lines.append(f"æè¿°: {desc}")
            
            # æ·»åŠ å‚æ•°è¯´æ˜
            if meta.input_schema and "properties" in meta.input_schema:
                params = []
                for param_name, param_info in meta.input_schema["properties"].items():
                    param_type = param_info.get("type", "string")
                    param_desc = param_info.get("description", "")
                    required = param_name in meta.input_schema.get("required", [])
                    params.append(f"  - {param_name} ({param_type}{'*' if required else ''}): {param_desc}")
                if params:
                    tool_lines.append("å‚æ•°:")
                    tool_lines.extend(params)
            tool_lines.append("")
        
        return "\n".join(tool_lines)
    
    async def _save_conversation(
        self,
        session_id: str,
        content: str,
        role: str,
    ) -> None:
        """ä¿å­˜å¯¹è¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": [], "tool_results": []}
        
        self.sessions[session_id]["history"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
        })
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.sessions[session_id]["history"]) > 50:
            self.sessions[session_id]["history"] = self.sessions[session_id]["history"][-50:]
    
    async def _save_tool_result(
        self,
        session_id: str,
        tool_name: str,
        args: Dict[str, Any],
        result: str,
        success: bool,
    ) -> None:
        """
        ä¿å­˜å·¥å…·è°ƒç”¨ç»“æœåˆ°ä¼šè¯è®°å¿†
        
        è¿™æ · AI å¯ä»¥åœ¨åç»­å¯¹è¯ä¸­"è®°ä½"ä¹‹å‰å·¥å…·è°ƒç”¨çš„ç»“æœ
        """
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": [], "tool_results": []}
        
        if "tool_results" not in self.sessions[session_id]:
            self.sessions[session_id]["tool_results"] = []
        
        # ä¿å­˜å·¥å…·ç»“æœ
        tool_record = {
            "tool": tool_name,
            "args": args,
            "result": result[:2000] if len(result) > 2000 else result,  # é™åˆ¶é•¿åº¦
            "success": success,
            "timestamp": datetime.now().isoformat(),
        }
        self.sessions[session_id]["tool_results"].append(tool_record)
        
        # åŒæ—¶æ·»åŠ åˆ°å¯¹è¯å†å²ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
        summary = self._summarize_tool_result(tool_name, args, result, success)
        self.sessions[session_id]["history"].append({
            "role": "tool",
            "tool_name": tool_name,
            "content": summary,
            "timestamp": datetime.now().isoformat(),
        })
        
        # é™åˆ¶å·¥å…·ç»“æœå†å²é•¿åº¦
        if len(self.sessions[session_id]["tool_results"]) > 20:
            self.sessions[session_id]["tool_results"] = self.sessions[session_id]["tool_results"][-20:]
        
        logger.debug(f"Tool result saved: {tool_name} -> {success}")
    
    def _summarize_tool_result(
        self,
        tool_name: str,
        args: Dict[str, Any],
        result: str,
        success: bool,
    ) -> str:
        """ç”Ÿæˆå·¥å…·ç»“æœçš„æ‘˜è¦ï¼ˆç”¨äºå¯¹è¯å†å²ï¼‰"""
        status = "æˆåŠŸ" if success else "å¤±è´¥"
        
        # æ ¹æ®å·¥å…·ç±»å‹ç”Ÿæˆæ‘˜è¦ - ä¿ç•™è¶³å¤Ÿçš„ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        if tool_name == "list_directory":
            path = args.get("path", ".")
            # æå–ç›®å½•/æ–‡ä»¶åˆ—è¡¨
            lines = result.strip().split("\n")
            count = len([l for l in lines if l.strip()])
            return f"[å·¥å…·: {tool_name}] åˆ—å‡º {path} ç›®å½•ï¼ŒåŒ…å« {count} ä¸ªé¡¹ç›®:\n{result[:2000]}"
        
        elif tool_name == "file_read_enhanced":
            path = args.get("path", "") or args.get("file_path", "")
            lines = result.count("\n") + 1
            return f"[å·¥å…·: {tool_name}] è¯»å–æ–‡ä»¶ {path} ({lines} è¡Œ):\n{result[:2000]}"
        
        elif tool_name == "shell_execute":
            cmd = args.get("command", "")
            return f"[å·¥å…·: {tool_name}] æ‰§è¡Œå‘½ä»¤ '{cmd[:50]}...' {status}:\n{result[:1500]}"
        
        elif tool_name == "codebase_search":
            query = args.get("query", "")
            return f"[å·¥å…·: {tool_name}] æœç´¢ '{query}' {status}:\n{result[:1500]}"
        
        else:
            # é€šç”¨æ‘˜è¦
            return f"[å·¥å…·: {tool_name}] {status}:\n{result[:1200]}"
    
    def _get_recent_tool_results(
        self,
        session_id: str,
        max_results: int = 5,
    ) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å·¥å…·è°ƒç”¨ç»“æœ"""
        if session_id not in self.sessions:
            return []
        
        tool_results = self.sessions[session_id].get("tool_results", [])
        return tool_results[-max_results:]
    
    def clear_session(self, session_id: str) -> None:
        """æ¸…é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]
        logger.info(f"Session {session_id} cleared")
    
    async def compact_session(
        self,
        session_id: str,
        force: bool = False,
    ) -> Optional[CompactionResult]:
        """
        å‹ç¼©ä¼šè¯å†å²
        
        å½“å¯¹è¯å†å²è¿‡é•¿æ—¶ï¼Œå‹ç¼©ä¸ºæ‘˜è¦ä»¥èŠ‚çœä¸Šä¸‹æ–‡ç©ºé—´ã€‚
        
        Args:
            session_id: ä¼šè¯ ID
            force: æ˜¯å¦å¼ºåˆ¶å‹ç¼©
        
        Returns:
            å‹ç¼©ç»“æœï¼Œå¦‚æœæœªå‹ç¼©åˆ™è¿”å› None
        """
        if session_id not in self.sessions:
            return None
        
        history = self.sessions[session_id].get("history", [])
        
        if not history:
            return None
        
        # å°†å†å²è½¬æ¢ä¸º ChatMessage æ ¼å¼
        from ..models.chat import ChatMessage, MessageRole
        
        messages = []
        for item in history:
            role_str = item.get("role", "user")
            if role_str == "user":
                role = MessageRole.USER
            elif role_str == "assistant":
                role = MessageRole.ASSISTANT
            elif role_str == "tool":
                role = MessageRole.SYSTEM
            else:
                role = MessageRole.SYSTEM
            
            messages.append(ChatMessage(
                role=role,
                content=item.get("content", ""),
                timestamp=datetime.fromisoformat(item.get("timestamp", datetime.now().isoformat())),
                metadata={"original_role": role_str},
            ))
        
        # æ‰§è¡Œå‹ç¼©
        compacted_messages, result = await self.session_compactor.compact(messages, force=force)
        
        if result.compacted_messages < result.original_messages:
            # æ›´æ–°ä¼šè¯å†å²
            new_history = []
            for msg in compacted_messages:
                original_role = msg.metadata.get("original_role", "system") if msg.metadata else "system"
                new_history.append({
                    "role": original_role,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat() if msg.timestamp else datetime.now().isoformat(),
                })
            
            self.sessions[session_id]["history"] = new_history
            
            logger.info(
                f"Session {session_id} compacted: "
                f"{result.original_messages} -> {result.compacted_messages} messages, "
                f"{result.compression_ratio:.1%} reduction"
            )
        
        return result
    
    async def auto_compact_if_needed(self, session_id: str) -> Optional[CompactionResult]:
        """
        å¦‚æœéœ€è¦åˆ™è‡ªåŠ¨å‹ç¼©
        
        åœ¨æ¯æ¬¡å¯¹è¯åè°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        
        Args:
            session_id: ä¼šè¯ ID
        
        Returns:
            å‹ç¼©ç»“æœï¼Œå¦‚æœæœªå‹ç¼©åˆ™è¿”å› None
        """
        if session_id not in self.sessions:
            return None
        
        history = self.sessions[session_id].get("history", [])
        
        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœå†å²è¶…è¿‡ 30 æ¡æ¶ˆæ¯ï¼Œè§¦å‘å‹ç¼©
        if len(history) > 30:
            return await self.compact_session(session_id)
        
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "active_sessions": len(self.sessions),
            "total_tools": len(self.tool_orchestrator.tools),
            "enabled_features": {
                "rag": self.enable_rag,
                "skills": self.enable_skills,
                "memory": self.enable_memory,
                "preferences": self.enable_preferences,
            },
        }


# å…¨å±€å®ä¾‹
_orchestrator: Optional[CursorStyleOrchestrator] = None


def get_orchestrator(
    llm_client=None,
    tools: Optional[List[Callable]] = None,
    **kwargs,
) -> CursorStyleOrchestrator:
    """è·å–ç¼–æ’å™¨å®ä¾‹"""
    global _orchestrator
    if _orchestrator is None:
        if llm_client is None:
            raise ValueError("First call requires llm_client")
        _orchestrator = CursorStyleOrchestrator(llm_client, tools, **kwargs)
    return _orchestrator

