# -*- coding: utf-8 -*-
"""
Cursor é£æ ¼ç¼–æ’å™¨ - Cursor-Style Orchestrator

è¿™æ˜¯ agentic_chatBot çš„æ ¸å¿ƒå¼•æ“ï¼Œæ•´åˆæ‰€æœ‰èƒ½åŠ›ï¼š
1. æ·±åº¦æ„å›¾è¯†åˆ« (IntentRecognizer)
2. è‡ªä¸»æ‰§è¡Œå¾ªç¯ (AgentLoop)
3. æ™ºèƒ½å·¥å…·ç¼–æ’ (ToolOrchestrator)
4. ä¸Šä¸‹æ–‡å·¥ç¨‹ (ContextManager)
5. ç”¨æˆ·åå¥½å­¦ä¹  (UserPreferences)
6. RAG çŸ¥è¯†æ£€ç´¢
7. æŠ€èƒ½ç³»ç»Ÿ (Skills)
8. è®°å¿†ç®¡ç† (Memory)

ç›®æ ‡ï¼šè®© ChatBot èƒ½åŠ›åª²ç¾ Cursorï¼
"""
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger
import asyncio
import traceback

# æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from .intent_recognizer import (
    IntentRecognizer, Intent, TaskType, RequiredCapability, get_intent_recognizer
)
from .agent_loop import (
    AgentLoop, ProgressUpdate, ExecutionPlan, LoopState
)
from .tool_orchestrator import (
    ToolOrchestrator, ToolSelection, get_tool_orchestrator
)
from .context_manager import (
    ContextManager, ContextSource, build_context
)
from .user_preferences import (
    UserPreferenceManager, get_preference_manager
)
from .planner import AgentPlanner
from .memory import MemoryManager
from .skills import SkillsManager, get_skills_manager


@dataclass
class ChatResponse:
    """
    èŠå¤©å“åº”
    
    åŒ…å«å®Œæ•´çš„å“åº”ä¿¡æ¯
    """
    content: str
    intent: Optional[Intent] = None
    used_tools: List[str] = field(default_factory=list)
    citations: List[Dict[str, str]] = field(default_factory=list)
    execution_steps: int = 0
    duration_ms: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "content": self.content,
            "intent": self.intent.to_dict() if self.intent else None,
            "used_tools": self.used_tools,
            "citations": self.citations,
            "execution_steps": self.execution_steps,
            "duration_ms": self.duration_ms,
            "metadata": self.metadata,
        }


@dataclass
class StreamChunk:
    """
    æµå¼è¾“å‡ºå—
    """
    type: str  # text, thinking, tool_call, tool_result, progress, complete, error
    content: str
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.type,
            "content": self.content,
            "metadata": self.metadata,
        }


class CursorStyleOrchestrator:
    """
    Cursor é£æ ¼ç¼–æ’å™¨
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ AI èƒ½åŠ›ï¼Œæä¾› Cursor çº§åˆ«çš„ ChatBot ä½“éªŒã€‚
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    orchestrator = CursorStyleOrchestrator(llm_client)
    
    # æµå¼å¯¹è¯
    async for chunk in orchestrator.chat_stream("åˆ†æè¿™ä¸ªé¡¹ç›®", session_id):
        if chunk.type == "text":
            print(chunk.content, end="")
        elif chunk.type == "thinking":
            print(f"ğŸ’­ {chunk.content}")
    
    # éæµå¼å¯¹è¯
    response = await orchestrator.chat("ä½ å¥½", session_id)
    print(response.content)
    ```
    """
    
    def __init__(
        self,
        llm_client,
        tools: Optional[List[Callable]] = None,
        enable_rag: bool = True,
        enable_skills: bool = True,
        enable_memory: bool = True,
        enable_preferences: bool = True,
        max_context_tokens: int = 8000,
    ):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            tools: å·¥å…·å‡½æ•°åˆ—è¡¨
            enable_rag: æ˜¯å¦å¯ç”¨ RAG
            enable_skills: æ˜¯å¦å¯ç”¨æŠ€èƒ½ç³»ç»Ÿ
            enable_memory: æ˜¯å¦å¯ç”¨è®°å¿†
            enable_preferences: æ˜¯å¦å¯ç”¨ç”¨æˆ·åå¥½å­¦ä¹ 
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡ Token æ•°
        """
        self.llm = llm_client
        self.max_context_tokens = max_context_tokens
        
        # åŠŸèƒ½å¼€å…³
        self.enable_rag = enable_rag
        self.enable_skills = enable_skills
        self.enable_memory = enable_memory
        self.enable_preferences = enable_preferences
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.intent_recognizer = get_intent_recognizer(llm_client)
        self.planner = AgentPlanner(llm_client)
        self.tool_orchestrator = get_tool_orchestrator(llm_client)
        self.preference_manager = get_preference_manager() if enable_preferences else None
        self.memory_manager = MemoryManager() if enable_memory else None
        self.skill_manager = get_skills_manager() if enable_skills else None
        
        # æ³¨å†Œå·¥å…·
        if tools:
            self.tool_orchestrator.register_many(tools)
        
        # ä¼šè¯çŠ¶æ€
        self.sessions: Dict[str, Dict[str, Any]] = {}
        
        logger.info(
            f"CursorStyleOrchestrator initialized: "
            f"RAG={enable_rag}, Skills={enable_skills}, "
            f"Memory={enable_memory}, Preferences={enable_preferences}"
        )
    
    async def chat_stream(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        æµå¼å¯¹è¯æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰
            files: å¼•ç”¨çš„æ–‡ä»¶ {path: content}
            rag_query: RAG æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ messageï¼‰
        
        Yields:
            StreamChunk æµå¼è¾“å‡ºå—
        """
        start_time = datetime.now()
        user_id = user_id or session_id
        used_tools = []
        
        try:
            # ==================== 1. æ„å›¾è¯†åˆ« ====================
            yield StreamChunk(type="thinking", content="ğŸ” åˆ†ææ‚¨çš„è¯·æ±‚...")
            
            intent = await self.intent_recognizer.recognize(
                message,
                history=self._get_conversation_history(session_id),
                available_tools=list(self.tool_orchestrator.tools.keys()),
            )
            
            logger.info(f"Intent: {intent.task_type.value}, complexity: {intent.complexity}")
            
            # ==================== 2. æ„å»ºä¸Šä¸‹æ–‡ ====================
            yield StreamChunk(type="thinking", content="ğŸ“š æ”¶é›†ç›¸å…³ä¿¡æ¯...")
            
            context = await self._build_context(
                message=message,
                session_id=session_id,
                user_id=user_id,
                intent=intent,
                files=files,
                rag_query=rag_query,
            )
            
            # ==================== 3. æ ¹æ®æ„å›¾é€‰æ‹©å¤„ç†ç­–ç•¥ ====================
            
            if intent.task_type == TaskType.CONVERSATION:
                # ç®€å•å¯¹è¯ï¼Œç›´æ¥å›å¤
                async for chunk in self._handle_conversation(message, context, user_id):
                    yield chunk
                    
            elif intent.is_multi_step or intent.complexity == "high":
                # å¤æ‚ä»»åŠ¡ï¼Œä½¿ç”¨ Agent Loop
                async for chunk in self._handle_complex_task(
                    message, intent, context, session_id, user_id
                ):
                    yield chunk
                    if chunk.metadata and chunk.metadata.get("tool"):
                        used_tools.append(chunk.metadata["tool"])
                        
            elif RequiredCapability.TOOLS in intent.required_capabilities:
                # éœ€è¦å·¥å…·ï¼Œä½¿ç”¨å·¥å…·ç¼–æ’
                async for chunk in self._handle_tool_task(
                    message, intent, context, user_id
                ):
                    yield chunk
                    if chunk.metadata and chunk.metadata.get("tool"):
                        used_tools.append(chunk.metadata["tool"])
                        
            else:
                # æ™®é€šä»»åŠ¡ï¼Œç›´æ¥ LLM å›å¤
                async for chunk in self._handle_simple_task(message, context, user_id):
                    yield chunk
            
            # ==================== 4. å­¦ä¹ å’Œè®°å½• ====================
            if self.enable_preferences:
                self.preference_manager.learn_from_message(user_id, message)
                for tool in used_tools:
                    self.preference_manager.learn_from_tool_usage(user_id, tool, True)
            
            # è®°å½•å¯¹è¯
            await self._save_conversation(session_id, message, "user")
            
            # è®¡ç®—è€—æ—¶
            duration = (datetime.now() - start_time).total_seconds() * 1000
            
            yield StreamChunk(
                type="complete",
                content="",
                metadata={
                    "duration_ms": int(duration),
                    "used_tools": used_tools,
                    "intent": intent.to_dict(),
                },
            )
            
        except Exception as e:
            logger.error(f"Chat error: {e}\n{traceback.format_exc()}")
            yield StreamChunk(
                type="error",
                content=f"âŒ å¤„ç†å¤±è´¥: {str(e)}",
            )
    
    async def chat(
        self,
        message: str,
        session_id: str,
        user_id: Optional[str] = None,
        files: Optional[Dict[str, str]] = None,
    ) -> ChatResponse:
        """
        éæµå¼å¯¹è¯æ¥å£
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            user_id: ç”¨æˆ· ID
            files: å¼•ç”¨çš„æ–‡ä»¶
        
        Returns:
            ChatResponse å®Œæ•´å“åº”
        """
        content_parts = []
        used_tools = []
        intent = None
        duration = 0
        
        async for chunk in self.chat_stream(message, session_id, user_id, files):
            if chunk.type == "text":
                content_parts.append(chunk.content)
            elif chunk.type in ["tool_call", "tool_result"]:
                if chunk.metadata and chunk.metadata.get("tool"):
                    used_tools.append(chunk.metadata["tool"])
            elif chunk.type == "complete" and chunk.metadata:
                intent_data = chunk.metadata.get("intent")
                if intent_data:
                    intent = Intent(**intent_data) if isinstance(intent_data, dict) else intent_data
                duration = chunk.metadata.get("duration_ms", 0)
        
        return ChatResponse(
            content="".join(content_parts),
            intent=intent,
            used_tools=list(set(used_tools)),
            duration_ms=duration,
        )
    
    async def _build_context(
        self,
        message: str,
        session_id: str,
        user_id: str,
        intent: Intent,
        files: Optional[Dict[str, str]] = None,
        rag_query: Optional[str] = None,
    ) -> str:
        """æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        cm = ContextManager(max_tokens=self.max_context_tokens)
        
        # 1. ç”¨æˆ·åå¥½ï¼ˆé£æ ¼æç¤ºï¼‰
        if self.enable_preferences:
            style_prompt = self.preference_manager.get_style_prompt(user_id)
            if style_prompt:
                cm.add(
                    content=style_prompt,
                    source=ContextSource.SYSTEM,
                    title="ç”¨æˆ·åå¥½",
                )
        
        # 2. æŠ€èƒ½æŒ‡ä»¤
        if self.enable_skills and RequiredCapability.SKILLS in intent.required_capabilities:
            relevant_skills = self.skill_manager.match_skills(message)
            for skill in relevant_skills[:2]:
                # Convert examples from Dict to formatted strings
                examples_str = []
                if skill.examples:
                    for ex in skill.examples:
                        if isinstance(ex, dict):
                            examples_str.append(f"ç”¨æˆ·: {ex.get('user', '')}\nåŠ©æ‰‹: {ex.get('assistant', '')}")
                        else:
                            examples_str.append(str(ex))
                cm.add_skill_instructions(
                    skill.name,
                    skill.instructions,
                    examples_str if examples_str else None,
                )
        
        # 3. RAG æ£€ç´¢
        if self.enable_rag and RequiredCapability.RAG in intent.required_capabilities:
            try:
                from ..rag import retriever
                rag_results = await retriever.retrieve(
                    query=rag_query or message,
                    top_k=5,
                )
                cm.add_rag_results(rag_results)
            except Exception as e:
                logger.warning(f"RAG retrieval failed: {e}")
        
        # 4. æ–‡ä»¶å†…å®¹
        if files:
            for path, content in files.items():
                cm.add_file_content(path, content)
        
        # 5. é•¿æœŸè®°å¿†
        if self.enable_memory and RequiredCapability.MEMORY in intent.required_capabilities:
            try:
                memories = await self.memory_manager.get_relevant_long_term_memory(
                    session_id, message
                )
                cm.add_memory(memories)
            except Exception as e:
                logger.warning(f"Memory retrieval failed: {e}")
        
        # 6. å¯¹è¯å†å²
        history = self._get_conversation_history(session_id)
        if history:
            cm.add_conversation_history(history, max_messages=5)
        
        return cm.build()
    
    async def _handle_conversation(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†æ™®é€šå¯¹è¯"""
        prompt = f"""
{context}

ç”¨æˆ·: {message}

è¯·å‹å¥½åœ°å›å¤ç”¨æˆ·ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤å¤±è´¥: {str(e)}")
    
    async def _handle_simple_task(
        self,
        message: str,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†ç®€å•ä»»åŠ¡"""
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®Œæˆç”¨æˆ·çš„è¯·æ±‚ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _handle_tool_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å·¥å…·ä»»åŠ¡"""
        # é€‰æ‹©å·¥å…·
        yield StreamChunk(type="thinking", content="ğŸ”§ é€‰æ‹©åˆé€‚çš„å·¥å…·...")
        
        selections = await self.tool_orchestrator.select_tools(message, max_tools=3)
        
        if not selections:
            # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œé™çº§ä¸ºæ™®é€šå›å¤
            async for chunk in self._handle_simple_task(message, context, user_id):
                yield chunk
            return
        
        # æ‰§è¡Œå·¥å…·
        results = []
        for sel in selections:
            yield StreamChunk(
                type="tool_call",
                content=f"ğŸ”§ æ‰§è¡Œ {sel.tool_name}...",
                metadata={"tool": sel.tool_name, "reason": sel.reason},
            )
            
            success, output = await self.tool_orchestrator.execute(
                sel.tool_name,
                sel.arguments,
            )
            
            results.append({
                "tool": sel.tool_name,
                "success": success,
                "output": output,
            })
            
            yield StreamChunk(
                type="tool_result",
                content=f"{'âœ…' if success else 'âŒ'} {sel.tool_name}: {str(output)[:200]}",
                metadata={"tool": sel.tool_name, "success": success},
            )
        
        # ç”Ÿæˆæœ€ç»ˆå›å¤
        result_text = "\n".join([
            f"- {r['tool']}: {r['output']}" for r in results
        ])
        
        prompt = f"""
{context}

ç”¨æˆ·è¯·æ±‚: {message}

å·¥å…·æ‰§è¡Œç»“æœ:
{result_text}

è¯·æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªå®Œæ•´çš„å›å¤ã€‚"""
        
        try:
            response = await self.llm.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
            )
            
            yield StreamChunk(type="text", content=response)
            
        except Exception as e:
            yield StreamChunk(type="error", content=f"å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def _handle_complex_task(
        self,
        message: str,
        intent: Intent,
        context: str,
        session_id: str,
        user_id: str,
    ) -> AsyncGenerator[StreamChunk, None]:
        """å¤„ç†å¤æ‚ä»»åŠ¡ï¼ˆä½¿ç”¨ Agent Loopï¼‰"""
        yield StreamChunk(
            type="thinking",
            content=f"ğŸ¯ æ£€æµ‹åˆ°å¤æ‚ä»»åŠ¡ (é¢„è®¡ {intent.estimated_steps} æ­¥)ï¼Œå¯åŠ¨ Agent æ¨¡å¼...",
        )
        
        # åˆ›å»º Agent Loop
        loop = AgentLoop(
            llm_client=self.llm,
            tools=self.tool_orchestrator.tools,
            planner=self.planner,
            max_steps=min(intent.estimated_steps + 5, 15),
        )
        
        # æ‰§è¡Œ
        final_content = ""
        
        async for update in loop.execute(message, intent, {"context": context}):
            if update.type == "thinking":
                yield StreamChunk(type="thinking", content=update.message)
                
            elif update.type == "action":
                tool_name = update.data.get("step", {}).get("tool_name", "") if update.data else ""
                yield StreamChunk(
                    type="tool_call",
                    content=update.message,
                    metadata={"tool": tool_name, "step": update.step},
                )
                
            elif update.type == "result":
                yield StreamChunk(
                    type="tool_result",
                    content=update.message,
                    metadata=update.data,
                )
                
            elif update.type == "progress":
                yield StreamChunk(
                    type="progress",
                    content=update.message,
                    metadata={
                        "step": update.step,
                        "total": update.total_steps,
                    },
                )
                
            elif update.type == "complete":
                final_content = update.message
                yield StreamChunk(type="text", content=update.message)
                
            elif update.type == "error":
                yield StreamChunk(type="error", content=update.message)
    
    def _get_conversation_history(
        self,
        session_id: str,
        max_messages: int = 10,
    ) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": []}
        
        history = self.sessions[session_id].get("history", [])
        return history[-max_messages:]
    
    async def _save_conversation(
        self,
        session_id: str,
        content: str,
        role: str,
    ) -> None:
        """ä¿å­˜å¯¹è¯"""
        if session_id not in self.sessions:
            self.sessions[session_id] = {"history": []}
        
        self.sessions[session_id]["history"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
        })
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.sessions[session_id]["history"]) > 50:
            self.sessions[session_id]["history"] = self.sessions[session_id]["history"][-50:]
    
    def clear_session(self, session_id: str) -> None:
        """æ¸…é™¤ä¼šè¯"""
        if session_id in self.sessions:
            del self.sessions[session_id]
        logger.info(f"Session {session_id} cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "active_sessions": len(self.sessions),
            "total_tools": len(self.tool_orchestrator.tools),
            "enabled_features": {
                "rag": self.enable_rag,
                "skills": self.enable_skills,
                "memory": self.enable_memory,
                "preferences": self.enable_preferences,
            },
        }


# å…¨å±€å®ä¾‹
_orchestrator: Optional[CursorStyleOrchestrator] = None


def get_orchestrator(
    llm_client=None,
    tools: Optional[List[Callable]] = None,
    **kwargs,
) -> CursorStyleOrchestrator:
    """è·å–ç¼–æ’å™¨å®ä¾‹"""
    global _orchestrator
    if _orchestrator is None:
        if llm_client is None:
            raise ValueError("First call requires llm_client")
        _orchestrator = CursorStyleOrchestrator(llm_client, tools, **kwargs)
    return _orchestrator

