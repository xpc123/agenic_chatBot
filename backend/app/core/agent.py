"""
æ ¸å¿ƒ Agent å¼•æ“ - ä¸»æ§åˆ¶å™¨
åŸºäº LangChain 1.0 create_agent

è¿™æ˜¯åº”ç”¨çš„æ ¸å¿ƒåè°ƒå™¨ï¼Œè´Ÿè´£:
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
2. å¤„ç† @è·¯å¾„å¼•ç”¨ï¼ˆContext Loadingï¼‰
3. RAG çŸ¥è¯†æ£€ç´¢
4. è°ƒç”¨ LangChain Agent æ‰§è¡Œ ReAct å¾ªç¯
5. ç®¡ç†å¯¹è¯è®°å¿†
6. ç”Ÿæˆå¹¶è¿”å›å“åº”
"""
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from loguru import logger
from datetime import datetime

from ..models.chat import ChatMessage, MessageRole
from ..config import settings
from .memory import MemoryManager
from .executor import ToolExecutor
from .context_loader import ContextLoader
from .langchain_agent import LangChainAgent, AgentContext
from .tools import calculator, get_current_time, search_web, get_basic_tools
from ..llm import get_llm_client


class AgentEngine:
    """
    æ ¸å¿ƒ Agent å¼•æ“
    
    åŸºäº LangChain 1.0 create_agent å®ç°ï¼Œæ˜¯åº”ç”¨çš„ä¸»è¦å…¥å£ç‚¹ã€‚
    
    æ¶æ„è¯´æ˜:
    - ä½¿ç”¨ LangChainAgent ä½œä¸ºåº•å±‚æ‰§è¡Œå¼•æ“
    - é€šè¿‡ Middleware å®ç°ä¸Šä¸‹æ–‡æ³¨å…¥ã€é”™è¯¯å¤„ç†ã€å†å²å‹ç¼©ç­‰
    - æ”¯æŒ RAG æ£€ç´¢å¢å¼º
    - æ”¯æŒ @è·¯å¾„å¼•ç”¨åŠ è½½æœ¬åœ°æ–‡ä»¶
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    engine = AgentEngine(memory_manager=memory)
    
    async for chunk in engine.chat("ä½ å¥½", session_id="123"):
        print(chunk)
    ```
    """
    
    def __init__(
        self,
        memory_manager: MemoryManager,
        tool_executor: Optional[ToolExecutor] = None,
        context_loader: Optional[ContextLoader] = None,
        tools: Optional[List[Callable]] = None,
        enable_summarization: bool = True,
        enable_pii_filter: bool = False,
        enable_human_in_loop: bool = False,
        human_approval_tools: Optional[List[str]] = None,
        enable_todo_list: bool = False,
    ):
        """
        åˆå§‹åŒ– Agent å¼•æ“
        
        Args:
            memory_manager: è®°å¿†ç®¡ç†å™¨
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼ˆå¯é€‰ï¼Œç”¨äº MCP å·¥å…·ï¼‰
            context_loader: ä¸Šä¸‹æ–‡åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            tools: é¢å¤–çš„å·¥å…·åˆ—è¡¨
            enable_summarization: æ˜¯å¦å¯ç”¨å¯¹è¯å†å²å‹ç¼©
            enable_pii_filter: æ˜¯å¦å¯ç”¨ PII è¿‡æ»¤
            enable_human_in_loop: æ˜¯å¦å¯ç”¨äººå·¥å®¡æ‰¹
            human_approval_tools: éœ€è¦äººå·¥å®¡æ‰¹çš„å·¥å…·åç§°
            enable_todo_list: æ˜¯å¦å¯ç”¨ä»»åŠ¡åˆ—è¡¨
        """
        self.memory = memory_manager
        self.executor = tool_executor
        self.context_loader = context_loader or ContextLoader()
        
        # æ„å»ºå·¥å…·åˆ—è¡¨ï¼šå†…ç½®å·¥å…· + è‡ªå®šä¹‰å·¥å…· + MCP å·¥å…·
        all_tools = self._build_tools(tools)
        
        # åˆå§‹åŒ– LangChain Agentï¼ˆä½¿ç”¨ LangChain 1.0 create_agentï¼‰
        self.langchain_agent = LangChainAgent(
            tools=all_tools,
            model=settings.OPENAI_MODEL,
            enable_summarization=enable_summarization,
            enable_pii_filter=enable_pii_filter,
            enable_human_in_loop=enable_human_in_loop,
            human_approval_tools=human_approval_tools,
            enable_todo_list=enable_todo_list,
            max_iterations=settings.MAX_ITERATIONS,
        )
        
        self.enable_path_reference = settings.ENABLE_PATH_REFERENCE
        
        logger.info(
            f"AgentEngine initialized with LangChain 1.0 create_agent, "
            f"tools={len(all_tools)}, path_reference={self.enable_path_reference}"
        )
    
    def _build_tools(self, custom_tools: Optional[List[Callable]] = None) -> List[Callable]:
        """
        æ„å»ºå·¥å…·åˆ—è¡¨
        
        ä¼˜å…ˆçº§:
        1. å†…ç½®å·¥å…· (calculator, get_current_time, etc.)
        2. è‡ªå®šä¹‰å·¥å…· (ç”¨æˆ·ä¼ å…¥)
        3. MCP å·¥å…· (å¦‚æœé…ç½®äº†)
        """
        # å†…ç½®å·¥å…·
        builtin_tools = [calculator, get_current_time, search_web]
        
        # è‡ªå®šä¹‰å·¥å…·
        user_tools = custom_tools or []
        
        # MCP å·¥å…· (ä» executor è·å–)
        mcp_tools = []
        if self.executor:
            try:
                mcp_tools = self.executor.get_langchain_tools()
            except Exception as e:
                logger.warning(f"Failed to load MCP tools: {e}")
        
        return builtin_tools + user_tools + mcp_tools
    
    async def chat(
        self,
        message: str,
        session_id: str,
        stream: bool = True,
        use_rag: bool = True,
        context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ä¸»å¯¹è¯æ–¹æ³• - æµå¼è¾“å‡º
        
        å¤„ç†æµç¨‹:
        1. å¤„ç† @è·¯å¾„å¼•ç”¨
        2. RAG æ£€ç´¢
        3. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        4. è°ƒç”¨ Agent
        5. ä¿å­˜ AI å›å¤
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            stream: æ˜¯å¦æµå¼è¾“å‡ºï¼ˆå§‹ç»ˆä¸º Trueï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ£€ç´¢
            context: é¢å¤–ä¸Šä¸‹æ–‡
        
        Yields:
            å“åº”å— {"type": "text|tool_call|tool_result|context|sources|error", ...}
        """
        logger.info(f"Processing message for session {session_id}: {message[:50]}...")
        
        # 1. å¤„ç† @è·¯å¾„å¼•ç”¨
        path_context = None
        if self.enable_path_reference:
            path_context = await self._load_path_references(message)
            if path_context:
                yield {
                    "type": "context",
                    "content": f"ğŸ“ åŠ è½½äº† {path_context.get('references_count', 0)} ä¸ªå¼•ç”¨",
                    "metadata": {
                        "contexts": path_context.get("contexts", []),
                    }
                }
        
        # 2. RAG æ£€ç´¢
        rag_results = None
        if use_rag:
            rag_data = await self._retrieve_knowledge(message, session_id)
            if rag_data:
                rag_results = rag_data["sources"]
                yield {
                    "type": "sources",
                    "content": rag_results,
                    "metadata": {"count": len(rag_results)}
                }
        
        # 3. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
        await self.memory.add_message(
            session_id,
            ChatMessage(role=MessageRole.USER, content=message)
        )
        
        # 4. ä½¿ç”¨ LangChain Agent æ‰§è¡Œ
        final_response = ""
        agent_context = AgentContext(
            session_id=session_id,
            user_id=context.get("user_id", "") if context else "",
            rag_enabled=use_rag,
            extra_context=context,
        )
        
        async for chunk in self.langchain_agent.chat(
            message=message,
            session_id=session_id,
            rag_results=rag_results,
            path_context=path_context,
            context=agent_context,
        ):
            yield chunk
            # ç´¯ç§¯æœ€ç»ˆå›å¤
            if chunk.get("type") == "text":
                final_response = chunk.get("content", "")
        
        # 5. ä¿å­˜ AI å›å¤åˆ°è®°å¿†
        if final_response:
            await self.memory.add_message(
                session_id,
                ChatMessage(role=MessageRole.ASSISTANT, content=final_response)
            )
            logger.info(f"Response saved for session {session_id}")
    
    async def _load_path_references(self, message: str) -> Optional[Dict[str, Any]]:
        """
        å¤„ç† @è·¯å¾„å¼•ç”¨
        
        æ”¯æŒæ ¼å¼:
        - @/path/to/file.py (ç»å¯¹è·¯å¾„)
        - @./relative/path.md (ç›¸å¯¹è·¯å¾„)
        - @path/to/directory/ (ç›®å½•)
        """
        try:
            loaded_context = await self.context_loader.load_context_from_message(message)
            if loaded_context.get("contexts"):
                formatted_context = await self.context_loader.format_context_for_llm(
                    loaded_context["contexts"]
                )
                loaded_context["formatted"] = formatted_context
                return loaded_context
        except Exception as e:
            logger.error(f"Failed to load path references: {e}")
        
        return None
    
    async def _retrieve_knowledge(
        self, 
        query: str, 
        session_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        ä» RAG ç³»ç»Ÿæ£€ç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            session_id: ä¼šè¯ ID
        
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« sources å’Œ context
        """
        try:
            from ..rag.retriever import retriever
            
            results = await retriever.retrieve(
                query=query,
                top_k=settings.TOP_K_RETRIEVAL,
            )
            
            if not results:
                return None
            
            return {
                "sources": results,
                "context": "\n\n".join([r.get("content", "") for r in results]),
            }
        except ImportError:
            logger.debug("RAG retriever not available")
            return None
        except Exception as e:
            logger.error(f"RAG retrieval error: {e}")
            return None
    
    def add_tool(self, tool_func: Callable):
        """
        åŠ¨æ€æ·»åŠ å·¥å…·åˆ° Agent
        
        Args:
            tool_func: ä½¿ç”¨ @tool è£…é¥°å™¨çš„å‡½æ•°
        """
        self.langchain_agent.add_tool(tool_func)
        logger.info(f"Tool added to AgentEngine: {tool_func.__name__}")
    
    async def invoke(
        self,
        message: str,
        session_id: str,
        use_rag: bool = True,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        åŒæ­¥è°ƒç”¨æ¥å£ï¼ˆéæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG
            context: é¢å¤–ä¸Šä¸‹æ–‡
        
        Returns:
            æœ€ç»ˆå›å¤æ–‡æœ¬
        """
        # å¤„ç† @è·¯å¾„å¼•ç”¨
        path_context = None
        if self.enable_path_reference:
            path_context = await self._load_path_references(message)
        
        # RAG æ£€ç´¢
        rag_results = None
        if use_rag:
            rag_data = await self._retrieve_knowledge(message, session_id)
            if rag_data:
                rag_results = rag_data["sources"]
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        await self.memory.add_message(
            session_id,
            ChatMessage(role=MessageRole.USER, content=message)
        )
        
        # è°ƒç”¨ Agent
        agent_context = AgentContext(
            session_id=session_id,
            user_id=context.get("user_id", "") if context else "",
            rag_enabled=use_rag,
            extra_context=context,
        )
        
        response = self.langchain_agent.invoke(
            message=message,
            session_id=session_id,
            rag_results=rag_results,
            path_context=path_context,
            context=agent_context,
        )
        
        # ä¿å­˜å›å¤
        if response:
            await self.memory.add_message(
                session_id,
                ChatMessage(role=MessageRole.ASSISTANT, content=response)
            )
        
        return response
    
    async def clear_history(self, session_id: str) -> bool:
        """
        æ¸…é™¤ä¼šè¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            await self.memory.clear_session(session_id)
            logger.info(f"Session history cleared: {session_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to clear session history: {e}")
            return False
    
    async def get_history(
        self, 
        session_id: str, 
        max_messages: Optional[int] = None
    ) -> List[ChatMessage]:
        """
        è·å–ä¼šè¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
            max_messages: æœ€å¤§æ¶ˆæ¯æ•°
        
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        return await self.memory.get_conversation_history(
            session_id, 
            max_messages=max_messages
        )
