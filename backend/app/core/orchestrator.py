# -*- coding: utf-8 -*-
"""
å¯¹è¯åè°ƒå™¨ - ä¸šåŠ¡ç¼–æ’å±‚

è¿™æ˜¯åº”ç”¨çš„æ ¸å¿ƒåè°ƒå™¨ï¼ˆOrchestratorï¼‰ï¼Œè´Ÿè´£:
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
2. å¤„ç† @è·¯å¾„å¼•ç”¨ï¼ˆContext Loadingï¼‰
3. RAG çŸ¥è¯†æ£€ç´¢
4. ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆContext Engineeringï¼‰
5. è°ƒç”¨ ExecutorAgent æ‰§è¡Œ ReAct å¾ªç¯
6. ç®¡ç†å¯¹è¯è®°å¿†
7. ç”Ÿæˆå¹¶è¿”å›å“åº”

æ¶æ„è¯´æ˜:
- Orchestrator æ˜¯é«˜å±‚ä¸šåŠ¡åè°ƒå™¨ï¼ˆä¸æ˜¯ Agentï¼‰
- ExecutorAgent æ˜¯åº•å±‚ Agent æ‰§è¡Œå¼•æ“ï¼ˆçœŸæ­£çš„ Agentï¼‰
- ContextManager ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ä¸Šä¸‹æ–‡æ¥æº
"""
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from loguru import logger
from datetime import datetime

from ..models.chat import ChatMessage, MessageRole
from ..config import settings
from .memory import MemoryManager
from .tool_executor import ToolExecutor
from .context_loader import ContextLoader
from .context_manager import ContextManager
from .agent_engine import ExecutorAgent, AgentContext
from .tools import get_current_time, get_basic_tools, run_python_code
from ..llm import get_llm_client


class Orchestrator:
    """
    å¯¹è¯åè°ƒå™¨ - ä¸šåŠ¡ç¼–æ’å±‚
    
    è´Ÿè´£åè°ƒå„ä¸ªæ¨¡å—å·¥ä½œï¼Œæ˜¯åº”ç”¨çš„ä¸»è¦å…¥å£ç‚¹ã€‚
    æ³¨æ„ï¼šè¿™ä¸æ˜¯ Agentï¼Œè€Œæ˜¯åè°ƒå™¨/ç¼–æ’å™¨ã€‚
    
    æ¶æ„è¯´æ˜:
    - ä½¿ç”¨ ExecutorAgent ä½œä¸ºåº•å±‚æ‰§è¡Œå¼•æ“ï¼ˆçœŸæ­£çš„ Agentï¼‰
    - ä½¿ç”¨ ContextManager ç»Ÿä¸€ç®¡ç†ä¸Šä¸‹æ–‡ï¼ˆContext Engineeringï¼‰
    - é€šè¿‡ Middleware å®ç°ä¸Šä¸‹æ–‡æ³¨å…¥ã€é”™è¯¯å¤„ç†ã€å†å²å‹ç¼©ç­‰
    - æ”¯æŒ RAG æ£€ç´¢å¢å¼º
    - æ”¯æŒ @è·¯å¾„å¼•ç”¨åŠ è½½æœ¬åœ°æ–‡ä»¶
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    orchestrator = Orchestrator(memory_manager=memory)
    
    async for chunk in orchestrator.chat("ä½ å¥½", session_id="123"):
        print(chunk)
    ```
    """
    
    def __init__(
        self,
        memory_manager: MemoryManager,
        tool_executor: Optional[ToolExecutor] = None,
        context_loader: Optional[ContextLoader] = None,
        tools: Optional[List[Callable]] = None,
        enable_summarization: bool = False,  # é»˜è®¤ç¦ç”¨ï¼Œéœ€è¦ OpenAI key
        enable_pii_filter: bool = False,
        enable_human_in_loop: bool = False,
        human_approval_tools: Optional[List[str]] = None,
        enable_todo_list: bool = False,
    ):
        """
        åˆå§‹åŒ–åè°ƒå™¨
        
        Args:
            memory_manager: è®°å¿†ç®¡ç†å™¨
            tool_executor: å·¥å…·æ‰§è¡Œå™¨ï¼ˆå¯é€‰ï¼Œç”¨äº MCP å·¥å…·ï¼‰
            context_loader: ä¸Šä¸‹æ–‡åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            tools: é¢å¤–çš„å·¥å…·åˆ—è¡¨
            enable_summarization: æ˜¯å¦å¯ç”¨å¯¹è¯å†å²å‹ç¼©
            enable_pii_filter: æ˜¯å¦å¯ç”¨ PII è¿‡æ»¤
            enable_human_in_loop: æ˜¯å¦å¯ç”¨äººå·¥å®¡æ‰¹
            human_approval_tools: éœ€è¦äººå·¥å®¡æ‰¹çš„å·¥å…·åç§°
            enable_todo_list: æ˜¯å¦å¯ç”¨ä»»åŠ¡åˆ—è¡¨
        """
        self.memory = memory_manager
        self.executor = tool_executor
        self.context_loader = context_loader or ContextLoader()
        
        # æ„å»ºå·¥å…·åˆ—è¡¨ï¼šå†…ç½®å·¥å…· + è‡ªå®šä¹‰å·¥å…· + MCP å·¥å…·
        all_tools = self._build_tools(tools)
        
        # åˆå§‹åŒ– Agent æ‰§è¡Œå™¨
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹å’Œæä¾›å•†
        provider = settings.LLM_PROVIDER
        model_name = settings.JEDAI_MODEL if provider == "jedai" else settings.OPENAI_MODEL
        
        self.agent_executor = ExecutorAgent(
            tools=all_tools,
            model=model_name,
            provider=provider,
            enable_summarization=enable_summarization,
            enable_pii_filter=enable_pii_filter,
            enable_human_in_loop=enable_human_in_loop,
            human_approval_tools=human_approval_tools,
            enable_todo_list=enable_todo_list,
            max_iterations=settings.MAX_ITERATIONS,
        )
        
        self.enable_path_reference = settings.ENABLE_PATH_REFERENCE
        
        # Context Engineering: ä¸Šä¸‹æ–‡ Token é¢„ç®—é…ç½®
        self.context_max_tokens = getattr(settings, 'CONTEXT_MAX_TOKENS', 8000)
        self.context_reserve_tokens = getattr(settings, 'CONTEXT_RESERVE_TOKENS', 2000)
        
        logger.info(
            f"Orchestrator initialized, "
            f"tools={len(all_tools)}, path_reference={self.enable_path_reference}, "
            f"context_budget={self.context_max_tokens}"
        )
    
    def _build_tools(self, custom_tools: Optional[List[Callable]] = None) -> List[Callable]:
        """
        æ„å»ºå·¥å…·åˆ—è¡¨
        
        ä¼˜å…ˆçº§:
        1. å†…ç½®å·¥å…· (get_current_time, run_python_code, etc.)
        2. è‡ªå®šä¹‰å·¥å…· (ç”¨æˆ·ä¼ å…¥)
        3. MCP å·¥å…· (å¦‚æœé…ç½®äº†)
        """
        # å†…ç½®å·¥å…·
        builtin_tools = [get_current_time, run_python_code]
        
        # è‡ªå®šä¹‰å·¥å…·
        user_tools = custom_tools or []
        
        # MCP å·¥å…· (ä» executor è·å–)
        mcp_tools = []
        if self.executor:
            try:
                mcp_tools = self.executor.get_langchain_tools()
            except Exception as e:
                logger.warning(f"Failed to get MCP tools: {e}")
        
        return builtin_tools + user_tools + mcp_tools
    
    async def chat(
        self,
        message: str,
        session_id: str,
        stream: bool = True,
        use_rag: bool = True,
        context: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ä¸»å¯¹è¯æ–¹æ³• - æµå¼è¾“å‡º
        
        å¤„ç†æµç¨‹:
        1. åˆ›å»º ContextManagerï¼ˆç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
        2. å¤„ç† @è·¯å¾„å¼•ç”¨
        3. RAG æ£€ç´¢
        4. è·å–å¯¹è¯å†å²
        5. è·å–ç”¨æˆ·åå¥½ï¼ˆé•¿æœŸè®°å¿†ï¼‰
        6. æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡
        7. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        8. è°ƒç”¨ Agent
        9. ä¿å­˜ AI å›å¤
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            stream: æ˜¯å¦æµå¼è¾“å‡ºï¼ˆå§‹ç»ˆä¸º Trueï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG æ£€ç´¢
            context: é¢å¤–ä¸Šä¸‹æ–‡
        
        Yields:
            å“åº”å— {"type": "text|tool_call|tool_result|context|sources|error", ...}
        """
        logger.info(f"Processing message for session {session_id}: {message[:50]}...")
        
        # ========== Context Engineering: ç»Ÿä¸€ä¸Šä¸‹æ–‡ç®¡ç† ==========
        ctx_manager = ContextManager(
            max_tokens=self.context_max_tokens,
            reserve_tokens=self.context_reserve_tokens,
        )
        
        # 1. å¤„ç† @è·¯å¾„å¼•ç”¨ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        path_context = None
        if self.enable_path_reference:
            path_context = await self._load_path_references(message)
            if path_context:
                ctx_manager.add_path_references(path_context)
                yield {
                    "type": "context",
                    "content": f"ğŸ“ åŠ è½½äº† {path_context.get('references_count', 0)} ä¸ªå¼•ç”¨",
                    "metadata": {
                        "contexts": path_context.get("contexts", []),
                    }
                }
        
        # 2. RAG æ£€ç´¢
        rag_results = None
        if use_rag:
            rag_data = await self._retrieve_knowledge(message, session_id)
            if rag_data:
                rag_results = rag_data["sources"]
                ctx_manager.add_rag_results(rag_results)
                yield {
                    "type": "sources",
                    "content": rag_results,
                    "metadata": {"count": len(rag_results)}
                }
        
        # 3. è·å–å¯¹è¯å†å²
        conversation_history = await self.memory.get_history(session_id)
        if conversation_history:
            history_messages = [
                {"role": msg.role.value, "content": msg.content}
                for msg in conversation_history
            ]
            ctx_manager.add_conversation_history(history_messages)
        
        # 4. è·å–ç”¨æˆ·åå¥½ï¼ˆé•¿æœŸè®°å¿†ï¼Œå¦‚æœæ”¯æŒï¼‰
        if hasattr(self.memory, 'get_user_preferences'):
            user_id = context.get("user_id", "") if context else ""
            if user_id:
                preferences = await self.memory.get_user_preferences(user_id)
                if preferences:
                    ctx_manager.add_user_preferences(preferences)
        
        # 5. æ„å»ºç»Ÿä¸€ä¸Šä¸‹æ–‡
        unified_context = ctx_manager.build()
        context_stats = ctx_manager.get_stats()
        logger.info(f"Context built: {context_stats['total_items']} items, "
                   f"{context_stats['utilization_percent']} utilization")
        
        # 6. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
        await self.memory.add_message(
            session_id,
            ChatMessage(role=MessageRole.USER, content=message)
        )
        
        # 7. ä½¿ç”¨ Agent æ‰§è¡Œ
        final_response = ""
        agent_context = AgentContext(
            session_id=session_id,
            user_id=context.get("user_id", "") if context else "",
            rag_enabled=use_rag,
            extra_context=context,
        )
        
        async for chunk in self.agent_executor.chat(
            message=message,
            session_id=session_id,
            unified_context=unified_context,  # ä½¿ç”¨ç»Ÿä¸€ä¸Šä¸‹æ–‡
            context=agent_context,
        ):
            yield chunk
            # ç´¯ç§¯æœ€ç»ˆå›å¤
            if chunk.get("type") == "text":
                final_response = chunk.get("content", "")
        
        # 8. ä¿å­˜ AI å›å¤åˆ°è®°å¿†
        if final_response:
            await self.memory.add_message(
                session_id,
                ChatMessage(role=MessageRole.ASSISTANT, content=final_response)
            )
            logger.info(f"Response saved for session {session_id}")
    
    async def _load_path_references(self, message: str) -> Optional[Dict[str, Any]]:
        """
        å¤„ç† @è·¯å¾„å¼•ç”¨
        
        æ”¯æŒæ ¼å¼:
        - @/path/to/file.py (ç»å¯¹è·¯å¾„)
        - @./relative/path.md (ç›¸å¯¹è·¯å¾„)
        - @path/to/directory/ (ç›®å½•)
        """
        try:
            loaded_context = await self.context_loader.load_context_from_message(message)
            if loaded_context.get("contexts"):
                formatted_context = await self.context_loader.format_context_for_llm(
                    loaded_context["contexts"]
                )
                loaded_context["formatted"] = formatted_context
                return loaded_context
        except Exception as e:
            logger.error(f"Failed to load path references: {e}")
        
        return None
    
    async def _retrieve_knowledge(
        self, 
        query: str, 
        session_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        ä» RAG ç³»ç»Ÿæ£€ç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            session_id: ä¼šè¯ ID
        
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« sources å’Œ context
        """
        try:
            from ..rag.retriever import retriever
            
            results = await retriever.retrieve(
                query=query,
                top_k=settings.TOP_K_RETRIEVAL,
            )
            
            if not results:
                return None
            
            return {
                "sources": results,
                "context": "\n\n".join([r.get("content", "") for r in results]),
            }
        except ImportError:
            logger.debug("RAG retriever not available")
            return None
        except Exception as e:
            logger.error(f"RAG retrieval error: {e}")
            return None
    
    def add_tool(self, tool_func: Callable):
        """
        åŠ¨æ€æ·»åŠ å·¥å…·åˆ° Agent
        
        Args:
            tool_func: ä½¿ç”¨ @tool è£…é¥°å™¨çš„å‡½æ•°
        """
        self.agent_executor.add_tool(tool_func)
        logger.info(f"Tool added to Orchestrator: {tool_func.__name__}")
    
    async def invoke(
        self,
        message: str,
        session_id: str,
        use_rag: bool = True,
        context: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        åŒæ­¥è°ƒç”¨æ¥å£ï¼ˆéæµå¼ï¼‰
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ ID
            use_rag: æ˜¯å¦ä½¿ç”¨ RAG
            context: é¢å¤–ä¸Šä¸‹æ–‡
        
        Returns:
            æœ€ç»ˆå›å¤æ–‡æœ¬
        """
        # å¤„ç† @è·¯å¾„å¼•ç”¨
        path_context = None
        if self.enable_path_reference:
            path_context = await self._load_path_references(message)
        
        # RAG æ£€ç´¢
        rag_results = None
        if use_rag:
            rag_data = await self._retrieve_knowledge(message, session_id)
            if rag_data:
                rag_results = rag_data["sources"]
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        await self.memory.add_message(
            session_id,
            ChatMessage(role=MessageRole.USER, content=message)
        )
        
        # è°ƒç”¨ Agent
        agent_context = AgentContext(
            session_id=session_id,
            user_id=context.get("user_id", "") if context else "",
            rag_enabled=use_rag,
            extra_context=context,
        )
        
        response = self.agent_executor.invoke(
            message=message,
            session_id=session_id,
            rag_results=rag_results,
            path_context=path_context,
            context=agent_context,
        )
        
        # ä¿å­˜å›å¤
        if response:
            await self.memory.add_message(
                session_id,
                ChatMessage(role=MessageRole.ASSISTANT, content=response)
            )
        
        return response
    
    async def clear_history(self, session_id: str) -> bool:
        """
        æ¸…é™¤ä¼šè¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            await self.memory.clear_session(session_id)
            logger.info(f"Session history cleared: {session_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to clear session history: {e}")
            return False
    
    async def get_history(
        self, 
        session_id: str, 
        max_messages: Optional[int] = None
    ) -> List[ChatMessage]:
        """
        è·å–ä¼šè¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
            max_messages: æœ€å¤§æ¶ˆæ¯æ•°
        
        Returns:
            æ¶ˆæ¯åˆ—è¡¨
        """
        return await self.memory.get_conversation_history(
            session_id, 
            max_messages=max_messages
        )


# å‘åå…¼å®¹åˆ«å
AgentOrchestrator = Orchestrator
