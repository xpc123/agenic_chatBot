# -*- coding: utf-8 -*-
"""
å¢å¼ºå·¥å…·é›† - Enhanced Tools

å‚è€ƒ OpenCode çš„å·¥å…·å®ç°ï¼š
1. è¯­ä¹‰ä»£ç æœç´¢ (codesearch) - ä½¿ç”¨ embedding è¿›è¡Œè¯­ä¹‰æœç´¢
2. å¤šæ–‡ä»¶ç¼–è¾‘ (multiedit) - æ‰¹é‡ç¼–è¾‘å¤šä¸ªæ–‡ä»¶
3. æ‰¹é‡æ“ä½œ (batch) - å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨
4. Grep å¢å¼º - æ”¯æŒæ­£åˆ™å’Œä¸Šä¸‹æ–‡
5. Glob æ–‡ä»¶åŒ¹é… - é«˜æ•ˆæ–‡ä»¶æŸ¥æ‰¾
"""
import os
import re
import asyncio
import fnmatch
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from datetime import datetime
from langchain.tools import tool
from loguru import logger
import json

from ..config import settings


# ==================== 1. è¯­ä¹‰ä»£ç æœç´¢ ====================

@dataclass
class CodeSearchResult:
    """ä»£ç æœç´¢ç»“æœ"""
    file_path: str
    content: str
    line_start: int
    line_end: int
    score: float
    context: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "file_path": self.file_path,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "score": self.score,
            "content": self.content,
            "context": self.context,
        }


class SemanticCodeSearch:
    """
    è¯­ä¹‰ä»£ç æœç´¢å™¨
    
    ä½¿ç”¨ embedding è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œæ‰¾åˆ°ä¸æŸ¥è¯¢è¯­ä¹‰ç›¸å…³çš„ä»£ç 
    """
    
    def __init__(self, workspace_path: Optional[str] = None):
        self.workspace_path = workspace_path or os.getcwd()
        self._embedding_client = None
        self._index: Dict[str, List[Dict]] = {}  # æ–‡ä»¶ -> chunks
    
    async def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„ embedding"""
        try:
            from ..rag.embeddings import embedding_generator
            return await embedding_generator.embed_text(text)
        except Exception as e:
            logger.warning(f"Embedding failed, using fallback: {e}")
            # é™çº§ï¼šä½¿ç”¨ç®€å•çš„è¯è¢‹æ¨¡å‹
            return self._simple_embedding(text)
    
    def _simple_embedding(self, text: str) -> List[float]:
        """ç®€å•çš„è¯è¢‹ embeddingï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        import hashlib
        # ä½¿ç”¨ hash ç”Ÿæˆä¼ª embedding
        words = text.lower().split()
        vec = [0.0] * 128
        for word in words:
            h = int(hashlib.md5(word.encode()).hexdigest(), 16)
            for i in range(128):
                vec[i] += ((h >> i) & 1) * 0.1
        # å½’ä¸€åŒ–
        norm = sum(v * v for v in vec) ** 0.5
        if norm > 0:
            vec = [v / norm for v in vec]
        return vec
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)
    
    async def search(
        self,
        query: str,
        file_patterns: Optional[List[str]] = None,
        top_k: int = 10,
        min_score: float = 0.3,
    ) -> List[CodeSearchResult]:
        """
        è¯­ä¹‰æœç´¢ä»£ç 
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            file_patterns: æ–‡ä»¶æ¨¡å¼åˆ—è¡¨ï¼ˆå¦‚ ["*.py", "*.js"]ï¼‰
            top_k: è¿”å›æ•°é‡
            min_score: æœ€å°ç›¸ä¼¼åº¦
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"Semantic code search: {query[:50]}...")
        
        # è·å–æŸ¥è¯¢ embedding
        query_embedding = await self._get_embedding(query)
        
        # æ”¶é›†æ–‡ä»¶
        files = self._collect_files(file_patterns)
        
        results = []
        
        for file_path in files[:100]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
            try:
                chunks = await self._get_file_chunks(file_path)
                
                for chunk in chunks:
                    chunk_embedding = await self._get_embedding(chunk["content"])
                    score = self._cosine_similarity(query_embedding, chunk_embedding)
                    
                    if score >= min_score:
                        results.append(CodeSearchResult(
                            file_path=file_path,
                            content=chunk["content"],
                            line_start=chunk["line_start"],
                            line_end=chunk["line_end"],
                            score=score,
                            context=chunk.get("context", ""),
                        ))
            except Exception as e:
                logger.debug(f"Error processing {file_path}: {e}")
                continue
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        return results[:top_k]
    
    def _collect_files(
        self,
        patterns: Optional[List[str]] = None,
    ) -> List[str]:
        """æ”¶é›†æ–‡ä»¶"""
        if patterns is None:
            patterns = ["*.py", "*.js", "*.ts", "*.java", "*.go", "*.rs", "*.cpp", "*.c", "*.h"]
        
        files = []
        workspace = Path(self.workspace_path)
        
        # æ’é™¤æ¨¡å¼
        exclude_dirs = {".git", "node_modules", "__pycache__", "venv", ".venv", "dist", "build"}
        
        for pattern in patterns:
            for file_path in workspace.rglob(pattern):
                # æ£€æŸ¥æ’é™¤ç›®å½•
                if any(exc in file_path.parts for exc in exclude_dirs):
                    continue
                if file_path.is_file():
                    files.append(str(file_path))
        
        return files
    
    async def _get_file_chunks(
        self,
        file_path: str,
        chunk_size: int = 20,
        overlap: int = 5,
    ) -> List[Dict]:
        """
        å°†æ–‡ä»¶åˆ†å—
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_size: æ¯å—è¡Œæ•°
            overlap: é‡å è¡Œæ•°
        
        Returns:
            å—åˆ—è¡¨
        """
        chunks = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                lines = f.readlines()
        except Exception:
            return []
        
        for i in range(0, len(lines), chunk_size - overlap):
            chunk_lines = lines[i:i + chunk_size]
            if not chunk_lines:
                continue
            
            content = "".join(chunk_lines)
            
            # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‡½æ•°/ç±»åï¼‰
            context = self._extract_context(lines, i)
            
            chunks.append({
                "content": content,
                "line_start": i + 1,
                "line_end": i + len(chunk_lines),
                "context": context,
            })
        
        return chunks
    
    def _extract_context(self, lines: List[str], start_line: int) -> str:
        """æå–ä¸Šä¸‹æ–‡ï¼ˆå‡½æ•°/ç±»å®šä¹‰ï¼‰"""
        context_patterns = [
            r"^\s*(def|class|function|async function)\s+(\w+)",
            r"^\s*(public|private|protected)?\s*(static)?\s*(void|int|string|async)?\s+(\w+)\s*\(",
        ]
        
        # å‘ä¸ŠæŸ¥æ‰¾æœ€è¿‘çš„å®šä¹‰
        for i in range(start_line, max(0, start_line - 50), -1):
            if i >= len(lines):
                continue
            line = lines[i]
            for pattern in context_patterns:
                match = re.search(pattern, line)
                if match:
                    return line.strip()
        
        return ""


# ==================== 2. å¤šæ–‡ä»¶ç¼–è¾‘ ====================

@dataclass
class FileEdit:
    """å•ä¸ªæ–‡ä»¶ç¼–è¾‘"""
    file_path: str
    old_content: str
    new_content: str
    description: str = ""


@dataclass
class MultiEditResult:
    """å¤šæ–‡ä»¶ç¼–è¾‘ç»“æœ"""
    success: List[str] = field(default_factory=list)
    failed: List[Dict[str, str]] = field(default_factory=list)
    
    @property
    def total(self) -> int:
        return len(self.success) + len(self.failed)
    
    @property
    def success_rate(self) -> float:
        if self.total == 0:
            return 0.0
        return len(self.success) / self.total


class MultiFileEditor:
    """
    å¤šæ–‡ä»¶ç¼–è¾‘å™¨
    
    æ”¯æŒæ‰¹é‡ç¼–è¾‘å¤šä¸ªæ–‡ä»¶
    """
    
    def __init__(self, workspace_path: Optional[str] = None):
        self.workspace_path = workspace_path or os.getcwd()
        self._backup_dir = os.path.join(self.workspace_path, ".agentic_chatbot", "backups")
    
    async def edit_files(
        self,
        edits: List[FileEdit],
        create_backup: bool = True,
        dry_run: bool = False,
    ) -> MultiEditResult:
        """
        æ‰¹é‡ç¼–è¾‘æ–‡ä»¶
        
        Args:
            edits: ç¼–è¾‘åˆ—è¡¨
            create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            dry_run: æ˜¯å¦æ¨¡æ‹Ÿè¿è¡Œ
        
        Returns:
            ç¼–è¾‘ç»“æœ
        """
        result = MultiEditResult()
        
        for edit in edits:
            try:
                success = await self._apply_edit(
                    edit,
                    create_backup=create_backup,
                    dry_run=dry_run,
                )
                
                if success:
                    result.success.append(edit.file_path)
                else:
                    result.failed.append({
                        "file": edit.file_path,
                        "error": "Edit not applied (content not found)",
                    })
                    
            except Exception as e:
                result.failed.append({
                    "file": edit.file_path,
                    "error": str(e),
                })
        
        return result
    
    async def _apply_edit(
        self,
        edit: FileEdit,
        create_backup: bool = True,
        dry_run: bool = False,
    ) -> bool:
        """åº”ç”¨å•ä¸ªç¼–è¾‘"""
        file_path = Path(edit.file_path)
        
        if not file_path.is_absolute():
            file_path = Path(self.workspace_path) / file_path
        
        # è¯»å–æ–‡ä»¶
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ—§å†…å®¹æ˜¯å¦å­˜åœ¨
        if edit.old_content not in content:
            return False
        
        # æ›¿æ¢å†…å®¹
        new_content = content.replace(edit.old_content, edit.new_content, 1)
        
        if dry_run:
            logger.info(f"[DRY RUN] Would edit: {file_path}")
            return True
        
        # åˆ›å»ºå¤‡ä»½
        if create_backup:
            await self._create_backup(file_path, content)
        
        # å†™å…¥æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        logger.info(f"Edited: {file_path}")
        return True
    
    async def _create_backup(self, file_path: Path, content: str) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        os.makedirs(self._backup_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{file_path.name}.{timestamp}.bak"
        backup_path = os.path.join(self._backup_dir, backup_name)
        
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return backup_path
    
    async def search_and_replace(
        self,
        pattern: str,
        replacement: str,
        file_patterns: Optional[List[str]] = None,
        is_regex: bool = False,
        dry_run: bool = False,
    ) -> MultiEditResult:
        """
        æœç´¢å¹¶æ›¿æ¢
        
        Args:
            pattern: æœç´¢æ¨¡å¼
            replacement: æ›¿æ¢å†…å®¹
            file_patterns: æ–‡ä»¶æ¨¡å¼
            is_regex: æ˜¯å¦ä½¿ç”¨æ­£åˆ™
            dry_run: æ˜¯å¦æ¨¡æ‹Ÿ
        
        Returns:
            ç¼–è¾‘ç»“æœ
        """
        if file_patterns is None:
            file_patterns = ["*.py", "*.js", "*.ts", "*.json", "*.md"]
        
        workspace = Path(self.workspace_path)
        result = MultiEditResult()
        
        for file_pattern in file_patterns:
            for file_path in workspace.rglob(file_pattern):
                if not file_path.is_file():
                    continue
                
                # æ’é™¤ç›®å½•
                if any(exc in str(file_path) for exc in [".git", "node_modules", "__pycache__"]):
                    continue
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if is_regex:
                        new_content = re.sub(pattern, replacement, content)
                    else:
                        new_content = content.replace(pattern, replacement)
                    
                    if new_content != content:
                        if not dry_run:
                            await self._create_backup(file_path, content)
                            with open(file_path, 'w', encoding='utf-8') as f:
                                f.write(new_content)
                        
                        result.success.append(str(file_path))
                        
                except Exception as e:
                    result.failed.append({
                        "file": str(file_path),
                        "error": str(e),
                    })
        
        return result


# ==================== 3. æ‰¹é‡æ“ä½œæ‰§è¡Œå™¨ ====================

@dataclass
class BatchOperation:
    """æ‰¹é‡æ“ä½œ"""
    tool_name: str
    args: Dict[str, Any]
    id: str = ""


@dataclass
class BatchResult:
    """æ‰¹é‡æ“ä½œç»“æœ"""
    results: List[Dict[str, Any]] = field(default_factory=list)
    total: int = 0
    success_count: int = 0
    failed_count: int = 0
    duration_ms: int = 0


class BatchExecutor:
    """
    æ‰¹é‡æ“ä½œæ‰§è¡Œå™¨
    
    å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨
    """
    
    def __init__(self, tool_registry: Optional[Dict[str, Callable]] = None):
        self.tools = tool_registry or {}
    
    def register_tool(self, name: str, func: Callable):
        """æ³¨å†Œå·¥å…·"""
        self.tools[name] = func
    
    async def execute(
        self,
        operations: List[BatchOperation],
        max_concurrent: int = 5,
    ) -> BatchResult:
        """
        æ‰¹é‡æ‰§è¡Œæ“ä½œ
        
        Args:
            operations: æ“ä½œåˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        
        Returns:
            æ‰§è¡Œç»“æœ
        """
        start_time = datetime.now()
        result = BatchResult(total=len(operations))
        
        # ä½¿ç”¨ semaphore é™åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def run_operation(op: BatchOperation) -> Dict[str, Any]:
            async with semaphore:
                try:
                    if op.tool_name not in self.tools:
                        return {
                            "id": op.id,
                            "success": False,
                            "error": f"Unknown tool: {op.tool_name}",
                        }
                    
                    tool_func = self.tools[op.tool_name]
                    
                    # è°ƒç”¨å·¥å…·
                    if asyncio.iscoroutinefunction(tool_func):
                        output = await tool_func(**op.args)
                    else:
                        output = tool_func(**op.args)
                    
                    return {
                        "id": op.id,
                        "success": True,
                        "output": output,
                    }
                    
                except Exception as e:
                    return {
                        "id": op.id,
                        "success": False,
                        "error": str(e),
                    }
        
        # å¹¶è¡Œæ‰§è¡Œ
        tasks = [run_operation(op) for op in operations]
        results = await asyncio.gather(*tasks)
        
        result.results = results
        result.success_count = sum(1 for r in results if r.get("success"))
        result.failed_count = sum(1 for r in results if not r.get("success"))
        result.duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        
        return result


# ==================== 4. Grep å¢å¼º ====================

@tool
def grep_enhanced(
    pattern: str,
    path: str = ".",
    file_pattern: str = "*",
    context_lines: int = 2,
    is_regex: bool = True,
    case_sensitive: bool = True,
    max_results: int = 50,
) -> str:
    """
    å¢å¼ºç‰ˆ Grep - åœ¨æ–‡ä»¶ä¸­æœç´¢æ¨¡å¼ã€‚
    
    æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ã€ä¸Šä¸‹æ–‡æ˜¾ç¤ºã€æ–‡ä»¶è¿‡æ»¤ã€‚
    
    Args:
        pattern: æœç´¢æ¨¡å¼ï¼ˆå­—ç¬¦ä¸²æˆ–æ­£åˆ™è¡¨è¾¾å¼ï¼‰
        path: æœç´¢è·¯å¾„ï¼Œé»˜è®¤å½“å‰ç›®å½•
        file_pattern: æ–‡ä»¶åæ¨¡å¼ï¼ˆå¦‚ "*.py"ï¼‰
        context_lines: æ˜¾ç¤ºåŒ¹é…è¡Œå‰åçš„è¡Œæ•°
        is_regex: æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        case_sensitive: æ˜¯å¦åŒºåˆ†å¤§å°å†™
        max_results: æœ€å¤§ç»“æœæ•°
    
    Returns:
        æœç´¢ç»“æœ
    
    Examples:
        >>> grep_enhanced("def main", path="src", file_pattern="*.py")
        >>> grep_enhanced("TODO|FIXME", is_regex=True)
    """
    try:
        search_path = Path(path)
        if not search_path.exists():
            return f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}"
        
        # ç¼–è¯‘æ­£åˆ™
        flags = 0 if case_sensitive else re.IGNORECASE
        if is_regex:
            regex = re.compile(pattern, flags)
        else:
            regex = re.compile(re.escape(pattern), flags)
        
        results = []
        files_searched = 0
        matches_found = 0
        
        # æ”¶é›†æ–‡ä»¶
        if search_path.is_file():
            files = [search_path]
        else:
            files = list(search_path.rglob(file_pattern))
        
        # æ’é™¤ç›®å½•
        exclude_dirs = {".git", "node_modules", "__pycache__", "venv", ".venv"}
        
        for file_path in files:
            if not file_path.is_file():
                continue
            if any(exc in str(file_path) for exc in exclude_dirs):
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    lines = f.readlines()
                
                files_searched += 1
                
                for i, line in enumerate(lines):
                    if regex.search(line):
                        matches_found += 1
                        
                        if matches_found > max_results:
                            break
                        
                        # è·å–ä¸Šä¸‹æ–‡
                        start = max(0, i - context_lines)
                        end = min(len(lines), i + context_lines + 1)
                        context = lines[start:end]
                        
                        # æ ¼å¼åŒ–è¾“å‡º
                        result_lines = []
                        for j, ctx_line in enumerate(context, start=start + 1):
                            prefix = ">" if j == i + 1 else " "
                            result_lines.append(f"{prefix} {j:4d} | {ctx_line.rstrip()}")
                        
                        results.append({
                            "file": str(file_path),
                            "line": i + 1,
                            "context": "\n".join(result_lines),
                        })
                
                if matches_found > max_results:
                    break
                    
            except Exception:
                continue
        
        if not results:
            return f"ğŸ” æœªæ‰¾åˆ°åŒ¹é…: '{pattern}'\næœç´¢äº† {files_searched} ä¸ªæ–‡ä»¶"
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_parts = [f"ğŸ” æœç´¢ç»“æœ: '{pattern}'\n"]
        output_parts.append(f"æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…ï¼ˆå…±æœç´¢ {files_searched} ä¸ªæ–‡ä»¶ï¼‰\n")
        
        for result in results:
            output_parts.append(f"\nğŸ“„ {result['file']}:{result['line']}")
            output_parts.append(f"```\n{result['context']}\n```")
        
        if matches_found > max_results:
            output_parts.append(f"\nâš ï¸ ç»“æœå·²æˆªæ–­ï¼Œå…±æœ‰è¶…è¿‡ {max_results} ä¸ªåŒ¹é…")
        
        return "\n".join(output_parts)
        
    except re.error as e:
        return f"âŒ æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}"
    except Exception as e:
        return f"âŒ æœç´¢å¤±è´¥: {e}"


# ==================== 5. Glob æ–‡ä»¶åŒ¹é… ====================

@tool
def glob_search(
    pattern: str,
    path: str = ".",
    max_results: int = 100,
    include_size: bool = True,
    sort_by: str = "name",
) -> str:
    """
    Glob æ–‡ä»¶æœç´¢ - ä½¿ç”¨æ¨¡å¼åŒ¹é…æŸ¥æ‰¾æ–‡ä»¶ã€‚
    
    Args:
        pattern: Glob æ¨¡å¼ï¼ˆå¦‚ "**/*.py", "src/**/*.ts"ï¼‰
        path: æœç´¢èµ·å§‹è·¯å¾„
        max_results: æœ€å¤§ç»“æœæ•°
        include_size: æ˜¯å¦æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        sort_by: æ’åºæ–¹å¼ - "name", "size", "mtime"
    
    Returns:
        åŒ¹é…çš„æ–‡ä»¶åˆ—è¡¨
    
    Examples:
        >>> glob_search("**/*.py")
        >>> glob_search("src/**/*.ts", sort_by="mtime")
    """
    try:
        search_path = Path(path)
        if not search_path.exists():
            return f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}"
        
        # æ‰§è¡Œ glob
        matches = list(search_path.glob(pattern))
        
        # è¿‡æ»¤æ–‡ä»¶
        files = [m for m in matches if m.is_file()]
        
        # æ’é™¤ç›®å½•
        exclude_dirs = {".git", "node_modules", "__pycache__", "venv", ".venv"}
        files = [f for f in files if not any(exc in str(f) for exc in exclude_dirs)]
        
        # æ’åº
        if sort_by == "size":
            files.sort(key=lambda f: f.stat().st_size, reverse=True)
        elif sort_by == "mtime":
            files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        else:
            files.sort(key=lambda f: str(f))
        
        # æˆªæ–­
        truncated = len(files) > max_results
        files = files[:max_results]
        
        if not files:
            return f"ğŸ” æœªæ‰¾åˆ°åŒ¹é…: '{pattern}'"
        
        # æ ¼å¼åŒ–è¾“å‡º
        output_parts = [f"ğŸ” Glob æœç´¢: '{pattern}'"]
        output_parts.append(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶" + (" (å·²æˆªæ–­)" if truncated else ""))
        output_parts.append("")
        
        for f in files:
            rel_path = f.relative_to(search_path) if f.is_relative_to(search_path) else f
            
            if include_size:
                size = f.stat().st_size
                if size < 1024:
                    size_str = f"{size}B"
                elif size < 1024 * 1024:
                    size_str = f"{size/1024:.1f}KB"
                else:
                    size_str = f"{size/1024/1024:.1f}MB"
                output_parts.append(f"  {rel_path} ({size_str})")
            else:
                output_parts.append(f"  {rel_path}")
        
        return "\n".join(output_parts)
        
    except Exception as e:
        return f"âŒ Glob æœç´¢å¤±è´¥: {e}"


# ==================== å·¥å…·å‡½æ•°å°è£… ====================

@tool
async def semantic_code_search(
    query: str,
    file_types: str = "py,js,ts",
    top_k: int = 5,
) -> str:
    """
    è¯­ä¹‰ä»£ç æœç´¢ - ä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥æ‰¾ç›¸å…³ä»£ç ã€‚
    
    è¿™ä¸ªå·¥å…·ä½¿ç”¨è¯­ä¹‰ç†è§£æ¥æŸ¥æ‰¾ä¸ä½ çš„æè¿°ç›¸å…³çš„ä»£ç ï¼Œ
    ä¸éœ€è¦çŸ¥é“ç¡®åˆ‡çš„å‡½æ•°åæˆ–å…³é”®å­—ã€‚
    
    Args:
        query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚ "å¤„ç†ç”¨æˆ·è®¤è¯çš„ä»£ç ", "æ•°æ®åº“è¿æ¥é€»è¾‘"ï¼‰
        file_types: æ–‡ä»¶ç±»å‹ï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚ "py,js,ts"ï¼‰
        top_k: è¿”å›ç»“æœæ•°é‡
    
    Returns:
        ç›¸å…³ä»£ç ç‰‡æ®µ
    
    Examples:
        >>> semantic_code_search("ç”¨æˆ·ç™»å½•éªŒè¯")
        >>> semantic_code_search("API è¯·æ±‚å¤„ç†", file_types="py")
    """
    try:
        searcher = SemanticCodeSearch()
        
        # è§£ææ–‡ä»¶ç±»å‹
        patterns = [f"*.{ext.strip()}" for ext in file_types.split(",")]
        
        results = await searcher.search(query, file_patterns=patterns, top_k=top_k)
        
        if not results:
            return f"ğŸ” æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„ä»£ç "
        
        output_parts = [f"ğŸ” è¯­ä¹‰æœç´¢: '{query}'", f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ä»£ç ç‰‡æ®µ", ""]
        
        for i, result in enumerate(results, 1):
            output_parts.append(f"### {i}. {result.file_path}")
            output_parts.append(f"è¡Œ {result.line_start}-{result.line_end} (ç›¸ä¼¼åº¦: {result.score:.2f})")
            if result.context:
                output_parts.append(f"ä¸Šä¸‹æ–‡: {result.context}")
            output_parts.append(f"```\n{result.content}\n```")
            output_parts.append("")
        
        return "\n".join(output_parts)
        
    except Exception as e:
        return f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}"


@tool
async def multi_file_edit(
    edits_json: str,
    dry_run: bool = False,
) -> str:
    """
    å¤šæ–‡ä»¶ç¼–è¾‘ - æ‰¹é‡ç¼–è¾‘å¤šä¸ªæ–‡ä»¶ã€‚
    
    Args:
        edits_json: ç¼–è¾‘åˆ—è¡¨çš„ JSON å­—ç¬¦ä¸²ï¼Œæ ¼å¼:
            [
                {"file": "path/to/file.py", "old": "æ—§å†…å®¹", "new": "æ–°å†…å®¹"},
                ...
            ]
        dry_run: æ˜¯å¦æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
    
    Returns:
        ç¼–è¾‘ç»“æœ
    
    Examples:
        >>> multi_file_edit('[{"file": "a.py", "old": "foo", "new": "bar"}]')
    """
    try:
        edits_data = json.loads(edits_json)
        
        edits = [
            FileEdit(
                file_path=e["file"],
                old_content=e["old"],
                new_content=e["new"],
                description=e.get("desc", ""),
            )
            for e in edits_data
        ]
        
        editor = MultiFileEditor()
        result = await editor.edit_files(edits, dry_run=dry_run)
        
        output_parts = ["ğŸ“ å¤šæ–‡ä»¶ç¼–è¾‘ç»“æœ"]
        output_parts.append(f"æˆåŠŸ: {len(result.success)}, å¤±è´¥: {len(result.failed)}")
        
        if result.success:
            output_parts.append("\nâœ… æˆåŠŸç¼–è¾‘:")
            for f in result.success:
                output_parts.append(f"  - {f}")
        
        if result.failed:
            output_parts.append("\nâŒ ç¼–è¾‘å¤±è´¥:")
            for f in result.failed:
                output_parts.append(f"  - {f['file']}: {f['error']}")
        
        if dry_run:
            output_parts.append("\nâš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œæœªå®é™…ä¿®æ”¹æ–‡ä»¶")
        
        return "\n".join(output_parts)
        
    except json.JSONDecodeError as e:
        return f"âŒ JSON è§£æé”™è¯¯: {e}"
    except Exception as e:
        return f"âŒ å¤šæ–‡ä»¶ç¼–è¾‘å¤±è´¥: {e}"


@tool
def search_and_replace_all(
    search: str,
    replace: str,
    file_pattern: str = "*.py",
    path: str = ".",
    is_regex: bool = False,
    dry_run: bool = True,
) -> str:
    """
    å…¨å±€æœç´¢æ›¿æ¢ - åœ¨å¤šä¸ªæ–‡ä»¶ä¸­æœç´¢å¹¶æ›¿æ¢ã€‚
    
    âš ï¸ é»˜è®¤ä¸ºæ¨¡æ‹Ÿè¿è¡Œï¼Œéœ€è¦è®¾ç½® dry_run=False æ‰ä¼šå®é™…ä¿®æ”¹ã€‚
    
    Args:
        search: æœç´¢å†…å®¹
        replace: æ›¿æ¢å†…å®¹
        file_pattern: æ–‡ä»¶æ¨¡å¼ï¼ˆå¦‚ "*.py"ï¼‰
        path: æœç´¢è·¯å¾„
        is_regex: æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
        dry_run: æ˜¯å¦æ¨¡æ‹Ÿè¿è¡Œ
    
    Returns:
        æ›¿æ¢ç»“æœ
    
    Examples:
        >>> search_and_replace_all("old_name", "new_name", file_pattern="*.py")
    """
    try:
        import asyncio
        
        editor = MultiFileEditor(workspace_path=path)
        
        # ä½¿ç”¨ asyncio.run æ‰§è¡Œå¼‚æ­¥å‡½æ•°
        loop = asyncio.new_event_loop()
        try:
            result = loop.run_until_complete(
                editor.search_and_replace(
                    pattern=search,
                    replacement=replace,
                    file_patterns=[file_pattern],
                    is_regex=is_regex,
                    dry_run=dry_run,
                )
            )
        finally:
            loop.close()
        
        output_parts = ["ğŸ”„ æœç´¢æ›¿æ¢ç»“æœ"]
        output_parts.append(f"æœç´¢: '{search}' -> æ›¿æ¢: '{replace}'")
        output_parts.append(f"ä¿®æ”¹æ–‡ä»¶æ•°: {len(result.success)}")
        
        if result.success:
            output_parts.append("\nä¿®æ”¹çš„æ–‡ä»¶:")
            for f in result.success[:20]:
                output_parts.append(f"  - {f}")
            if len(result.success) > 20:
                output_parts.append(f"  ... è¿˜æœ‰ {len(result.success) - 20} ä¸ªæ–‡ä»¶")
        
        if result.failed:
            output_parts.append("\nå¤±è´¥:")
            for f in result.failed[:5]:
                output_parts.append(f"  - {f['file']}: {f['error']}")
        
        if dry_run:
            output_parts.append("\nâš ï¸ æ¨¡æ‹Ÿè¿è¡Œ - è®¾ç½® dry_run=False ä»¥å®é™…ä¿®æ”¹")
        
        return "\n".join(output_parts)
        
    except Exception as e:
        return f"âŒ æœç´¢æ›¿æ¢å¤±è´¥: {e}"


# ==================== å·¥å…·é›†åˆ ====================

def get_enhanced_tools() -> List:
    """è·å–å¢å¼ºå·¥å…·é›†"""
    return [
        grep_enhanced,
        glob_search,
        semantic_code_search,
        multi_file_edit,
        search_and_replace_all,
    ]


__all__ = [
    "SemanticCodeSearch",
    "CodeSearchResult",
    "MultiFileEditor",
    "FileEdit",
    "MultiEditResult",
    "BatchExecutor",
    "BatchOperation",
    "BatchResult",
    "grep_enhanced",
    "glob_search",
    "semantic_code_search",
    "multi_file_edit",
    "search_and_replace_all",
    "get_enhanced_tools",
]


