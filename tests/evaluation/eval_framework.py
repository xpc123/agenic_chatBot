# -*- coding: utf-8 -*-
"""
Agentic ChatBot è¯„ä¼°æ¡†æ¶

åŸºäºä¸šç•Œæ ‡å‡†çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹æ ‡:
- RAGAS (RAG Assessment)
- DeepEval
- SWE-Bench æ€è·¯

è¯„ä¼°ç»´åº¦:
1. å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§ (Tool Use Accuracy)
2. å“åº”ç›¸å…³æ€§ (Response Relevancy)
3. ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡ (Context Utilization)
4. ä»»åŠ¡å®Œæˆç‡ (Task Completion Rate)
5. å“åº”å»¶è¿Ÿ (Latency)
6. é”™è¯¯å¤„ç† (Error Handling)

ç”¨æ³•:
    python -m tests.evaluation.eval_framework
"""
import time
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from agentic_sdk import ChatBot


# ============================================================================
# è¯„ä¼°æŒ‡æ ‡å®šä¹‰
# ============================================================================

class MetricType(Enum):
    """è¯„ä¼°æŒ‡æ ‡ç±»å‹"""
    TOOL_USE_ACCURACY = "tool_use_accuracy"      # å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§
    RESPONSE_RELEVANCY = "response_relevancy"    # å“åº”ç›¸å…³æ€§
    CONTEXT_UTILIZATION = "context_utilization"  # ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡
    TASK_COMPLETION = "task_completion"          # ä»»åŠ¡å®Œæˆç‡
    LATENCY = "latency"                          # å“åº”å»¶è¿Ÿ
    ERROR_HANDLING = "error_handling"            # é”™è¯¯å¤„ç†


@dataclass
class EvalCase:
    """è¯„ä¼°ç”¨ä¾‹"""
    id: str
    query: str
    expected_tools: List[str] = field(default_factory=list)  # æœŸæœ›è°ƒç”¨çš„å·¥å…·
    expected_keywords: List[str] = field(default_factory=list)  # æœŸæœ›å“åº”åŒ…å«çš„å…³é”®è¯
    ground_truth: str = ""  # æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
    context: Optional[str] = None  # é¢„è®¾ä¸Šä¸‹æ–‡
    max_latency_ms: int = 30000  # æœ€å¤§å»¶è¿Ÿ
    category: str = "general"


@dataclass
class EvalResult:
    """è¯„ä¼°ç»“æœ"""
    case_id: str
    metrics: Dict[str, float]
    response: str
    tool_calls: List[str]
    latency_ms: float
    errors: List[str]
    passed: bool
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class EvalReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    total_cases: int
    passed_cases: int
    failed_cases: int
    metrics_summary: Dict[str, float]
    category_breakdown: Dict[str, Dict[str, float]]
    results: List[EvalResult]
    timestamp: str = ""
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    def to_json(self, path: str) -> None:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)


# ============================================================================
# è¯„ä¼°ç”¨ä¾‹é›†
# ============================================================================

EVAL_CASES = [
    # ========== å·¥å…·ä½¿ç”¨è¯„ä¼° ==========
    EvalCase(
        id="tool_001",
        query="åˆ—å‡º /tmp ç›®å½•ä¸‹çš„æ–‡ä»¶",
        expected_tools=["list_directory"],
        expected_keywords=["ç›®å½•", "æ–‡ä»¶"],
        category="tool_use",
    ),
    EvalCase(
        id="tool_002",
        query="è¯»å– /etc/hostname æ–‡ä»¶çš„å†…å®¹",
        expected_tools=["file_read_enhanced"],
        expected_keywords=["hostname", "æ–‡ä»¶"],
        category="tool_use",
    ),
    EvalCase(
        id="tool_003",
        query="æ‰§è¡Œå‘½ä»¤ `whoami` æŸ¥çœ‹å½“å‰ç”¨æˆ·",
        expected_tools=["shell_execute"],
        expected_keywords=["ç”¨æˆ·"],
        category="tool_use",
    ),
    EvalCase(
        id="tool_004",
        query="å‘Šè¯‰æˆ‘å½“å‰ç³»ç»Ÿç¯å¢ƒä¿¡æ¯",
        expected_tools=["env_info"],
        expected_keywords=["ç³»ç»Ÿ", "ç¯å¢ƒ"],
        category="tool_use",
    ),
    
    # ========== å“åº”ç›¸å…³æ€§è¯„ä¼° ==========
    EvalCase(
        id="rel_001",
        query="ä»€ä¹ˆæ˜¯ Python è£…é¥°å™¨ï¼Ÿ",
        expected_keywords=["decorator", "å‡½æ•°", "åŒ…è£…", "@"],
        category="relevancy",
    ),
    EvalCase(
        id="rel_002",
        query="è§£é‡Š REST API çš„è®¾è®¡åŸåˆ™",
        expected_keywords=["HTTP", "èµ„æº", "çŠ¶æ€", "GET", "POST"],
        category="relevancy",
    ),
    
    # ========== ä¸Šä¸‹æ–‡åˆ©ç”¨è¯„ä¼° ==========
    EvalCase(
        id="ctx_001",
        query="æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ",
        context="ç”¨æˆ·ä¹‹å‰è¯´ï¼šæˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆã€‚",
        expected_keywords=["å¼ ä¸‰"],
        category="context",
    ),
    
    # ========== ä»»åŠ¡å®Œæˆè¯„ä¼° ==========
    EvalCase(
        id="task_001",
        query="å¸®æˆ‘å†™ä¸€ä¸ªè®¡ç®—é˜¶ä¹˜çš„ Python å‡½æ•°",
        expected_keywords=["def", "factorial", "return"],
        category="task_completion",
    ),
    EvalCase(
        id="task_002",
        query="åˆ†æ /ADE1/users/xpengche/project/ContexBuilder é¡¹ç›®çš„ç»“æ„",
        expected_tools=["list_directory"],
        expected_keywords=["src", "tests", "ç›®å½•", "é¡¹ç›®"],
        category="task_completion",
    ),
    
    # ========== é”™è¯¯å¤„ç†è¯„ä¼° ==========
    EvalCase(
        id="err_001",
        query="è¯»å– /ä¸å­˜åœ¨çš„è·¯å¾„/æ–‡ä»¶.txt",
        expected_keywords=["ä¸å­˜åœ¨", "é”™è¯¯", "æ‰¾ä¸åˆ°"],
        category="error_handling",
    ),
]


# ============================================================================
# è¯„ä¼°å™¨
# ============================================================================

class Evaluator:
    """è¯„ä¼°å™¨"""
    
    def __init__(self, bot: Optional[ChatBot] = None):
        self.bot = bot or ChatBot()
        self.results: List[EvalResult] = []
        
    def evaluate_case(self, case: EvalCase) -> EvalResult:
        """è¯„ä¼°å•ä¸ªç”¨ä¾‹"""
        start_time = time.time()
        response_parts = []
        tool_calls = []
        errors = []
        
        try:
            session_id = f"eval-{case.id}"
            
            # å¦‚æœæœ‰é¢„è®¾ä¸Šä¸‹æ–‡ï¼Œå…ˆå‘é€
            if case.context:
                for chunk in self.bot.chat_stream(case.context, session_id):
                    pass
            
            # å‘é€è¯„ä¼°æŸ¥è¯¢
            for chunk in self.bot.chat_stream(case.query, session_id):
                if chunk.type == "text":
                    response_parts.append(chunk.content or "")
                elif chunk.type == "tool_call":
                    tool_calls.append(chunk.content)
                elif chunk.type == "error":
                    errors.append(chunk.content)
                    
        except Exception as e:
            errors.append(str(e))
            
        latency_ms = (time.time() - start_time) * 1000
        response = "".join(response_parts)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics = self._calculate_metrics(case, response, tool_calls, latency_ms, errors)
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = self._judge_pass(case, metrics, errors)
        
        return EvalResult(
            case_id=case.id,
            metrics=metrics,
            response=response,
            tool_calls=tool_calls,
            latency_ms=latency_ms,
            errors=errors,
            passed=passed,
        )
    
    def _calculate_metrics(
        self,
        case: EvalCase,
        response: str,
        tool_calls: List[str],
        latency_ms: float,
        errors: List[str],
    ) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # 1. å·¥å…·ä½¿ç”¨å‡†ç¡®æ€§
        if case.expected_tools:
            tool_calls_text = " ".join(tool_calls).lower()
            matched = sum(1 for t in case.expected_tools if t.lower() in tool_calls_text)
            metrics["tool_use_accuracy"] = matched / len(case.expected_tools)
        else:
            metrics["tool_use_accuracy"] = 1.0  # ä¸éœ€è¦å·¥å…·åˆ™ä¸ºæ»¡åˆ†
            
        # 2. å“åº”ç›¸å…³æ€§ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        if case.expected_keywords:
            response_lower = response.lower()
            matched = sum(1 for kw in case.expected_keywords if kw.lower() in response_lower)
            metrics["response_relevancy"] = matched / len(case.expected_keywords)
        else:
            metrics["response_relevancy"] = 1.0 if response else 0.0
            
        # 3. å»¶è¿Ÿè¯„åˆ†
        if latency_ms <= case.max_latency_ms:
            # çº¿æ€§è¯„åˆ†ï¼šè¶Šå¿«è¶Šå¥½
            metrics["latency_score"] = max(0, 1 - (latency_ms / case.max_latency_ms) * 0.5)
        else:
            metrics["latency_score"] = 0.0
            
        # 4. é”™è¯¯å¤„ç†
        if case.category == "error_handling":
            # æœŸæœ›ä¼˜é›…å¤„ç†é”™è¯¯
            metrics["error_handling"] = 1.0 if response and not any("å¼‚å¸¸" in e for e in errors) else 0.0
        else:
            metrics["error_handling"] = 0.0 if errors else 1.0
            
        return metrics
    
    def _judge_pass(
        self,
        case: EvalCase,
        metrics: Dict[str, float],
        errors: List[str],
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦é€šè¿‡"""
        # å·¥å…·ä½¿ç”¨å¿…é¡»æ­£ç¡®
        if case.expected_tools and metrics.get("tool_use_accuracy", 0) < 0.5:
            return False
            
        # å“åº”ç›¸å…³æ€§è‡³å°‘ 50%
        if metrics.get("response_relevancy", 0) < 0.5:
            return False
            
        # éé”™è¯¯å¤„ç†æµ‹è¯•ä¸åº”æœ‰é”™è¯¯
        if case.category != "error_handling" and errors:
            return False
            
        return True
    
    def run_evaluation(self, cases: Optional[List[EvalCase]] = None) -> EvalReport:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        cases = cases or EVAL_CASES
        self.results = []
        
        print(f"\nğŸ“Š å¼€å§‹è¯„ä¼° ({len(cases)} ä¸ªç”¨ä¾‹)")
        print("=" * 60)
        
        for i, case in enumerate(cases, 1):
            print(f"[{i}/{len(cases)}] {case.id}: {case.query[:40]}...")
            result = self.evaluate_case(case)
            self.results.append(result)
            
            status = "âœ… PASS" if result.passed else "âŒ FAIL"
            print(f"  {status} (å»¶è¿Ÿ: {result.latency_ms:.0f}ms)")
            
            if not result.passed:
                print(f"  å·¥å…·å‡†ç¡®æ€§: {result.metrics.get('tool_use_accuracy', 0):.2f}")
                print(f"  å“åº”ç›¸å…³æ€§: {result.metrics.get('response_relevancy', 0):.2f}")
                
        return self._generate_report()
    
    def _generate_report(self) -> EvalReport:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        from datetime import datetime
        
        passed = sum(1 for r in self.results if r.passed)
        failed = len(self.results) - passed
        
        # è®¡ç®—å„æŒ‡æ ‡å¹³å‡å€¼
        metrics_summary = {}
        all_metrics = ["tool_use_accuracy", "response_relevancy", "latency_score", "error_handling"]
        for metric in all_metrics:
            values = [r.metrics.get(metric, 0) for r in self.results]
            metrics_summary[metric] = sum(values) / len(values) if values else 0
            
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_breakdown = {}
        for result in self.results:
            case = next(c for c in EVAL_CASES if c.id == result.case_id)
            cat = case.category
            if cat not in category_breakdown:
                category_breakdown[cat] = {"passed": 0, "failed": 0, "total": 0}
            category_breakdown[cat]["total"] += 1
            if result.passed:
                category_breakdown[cat]["passed"] += 1
            else:
                category_breakdown[cat]["failed"] += 1
                
        # è®¡ç®—é€šè¿‡ç‡
        for cat in category_breakdown:
            total = category_breakdown[cat]["total"]
            category_breakdown[cat]["pass_rate"] = category_breakdown[cat]["passed"] / total if total else 0
            
        report = EvalReport(
            total_cases=len(self.results),
            passed_cases=passed,
            failed_cases=failed,
            metrics_summary=metrics_summary,
            category_breakdown=category_breakdown,
            results=self.results,
            timestamp=datetime.now().isoformat(),
        )
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        print(f"æ€»è®¡: {report.total_cases} | é€šè¿‡: {passed} | å¤±è´¥: {failed}")
        print(f"é€šè¿‡ç‡: {passed/report.total_cases*100:.1f}%")
        
        print("\næŒ‡æ ‡æ‘˜è¦:")
        for metric, value in metrics_summary.items():
            print(f"  {metric}: {value:.2%}")
            
        print("\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for cat, stats in category_breakdown.items():
            print(f"  {cat}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.0%})")
            
        return report


# ============================================================================
# LLM-as-Judge è¯„ä¼°å™¨ï¼ˆé«˜çº§ï¼‰
# ============================================================================

class LLMJudge:
    """
    ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…
    
    ç±»ä¼¼ RAGAS çš„æ–¹æ³•ï¼Œä½¿ç”¨å¦ä¸€ä¸ª LLM æ¥è¯„ä¼°å“åº”è´¨é‡
    """
    
    def __init__(self, judge_bot: Optional[ChatBot] = None):
        self.judge = judge_bot or ChatBot()
        
    async def evaluate_response(
        self,
        query: str,
        response: str,
        ground_truth: Optional[str] = None,
    ) -> Dict[str, float]:
        """ä½¿ç”¨ LLM è¯„ä¼°å“åº”"""
        
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹ AI åŠ©æ‰‹çš„å›ç­”è´¨é‡ï¼Œç»™å‡º 0-1 çš„åˆ†æ•°ã€‚

ç”¨æˆ·é—®é¢˜: {query}

AI å›ç­”:
{response}

{"æ ‡å‡†ç­”æ¡ˆ: " + ground_truth if ground_truth else ""}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰ï¼š
1. å‡†ç¡®æ€§ (accuracy): ä¿¡æ¯æ˜¯å¦å‡†ç¡®
2. å®Œæ•´æ€§ (completeness): æ˜¯å¦å®Œæ•´å›ç­”äº†é—®é¢˜
3. æ¸…æ™°åº¦ (clarity): è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. æœ‰ç”¨æ€§ (helpfulness): å¯¹ç”¨æˆ·æ˜¯å¦æœ‰å¸®åŠ©

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{"accuracy": 0.X, "completeness": 0.X, "clarity": 0.X, "helpfulness": 0.X}}
"""
        
        try:
            result = ""
            for chunk in self.judge.chat_stream(prompt, "judge-session"):
                if chunk.type == "text":
                    result += chunk.content or ""
                    
            # è§£æ JSON
            import re
            json_match = re.search(r'\{[^}]+\}', result)
            if json_match:
                scores = json.loads(json_match.group())
                return scores
        except Exception as e:
            print(f"LLM Judge error: {e}")
            
        return {"accuracy": 0.5, "completeness": 0.5, "clarity": 0.5, "helpfulness": 0.5}


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Agentic ChatBot è¯„ä¼°")
    parser.add_argument("--category", "-c", help="åªè¯„ä¼°æŒ‡å®šç±»åˆ«")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„")
    
    args = parser.parse_args()
    
    evaluator = Evaluator()
    
    cases = EVAL_CASES
    if args.category:
        cases = [c for c in EVAL_CASES if c.category == args.category]
        
    report = evaluator.run_evaluation(cases)
    
    if args.output:
        report.to_json(args.output)
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()

