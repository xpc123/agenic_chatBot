# -*- coding: utf-8 -*-
"""
ChatBot èƒ½åŠ›è¯„ä¼°æ¡†æ¶

è¯„ä¼°ç»´åº¦ï¼š
1. å¯¹è¯è´¨é‡ - å›ç­”çš„ç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€å®Œæ•´æ€§
2. æ„å›¾ç†è§£ - æ­£ç¡®è¯†åˆ«ç”¨æˆ·æ„å›¾
3. ä¸Šä¸‹æ–‡ä¿æŒ - å¤šè½®å¯¹è¯ä¸­çš„è®°å¿†èƒ½åŠ›
4. å·¥å…·ä½¿ç”¨ - æ­£ç¡®è°ƒç”¨å’Œä½¿ç”¨å·¥å…·
5. çŸ¥è¯†æ£€ç´¢ - RAG æ£€ç´¢çš„å‡†ç¡®æ€§
6. å“åº”é€Ÿåº¦ - å»¶è¿Ÿå’Œååé‡
7. é”™è¯¯æ¢å¤ - å¤„ç†è¾¹ç¼˜æƒ…å†µçš„èƒ½åŠ›
"""
import json
import time
import requests
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
import statistics
from datetime import datetime


class EvaluationDimension(Enum):
    """è¯„ä¼°ç»´åº¦"""
    RELEVANCE = "relevance"           # ç›¸å…³æ€§
    ACCURACY = "accuracy"             # å‡†ç¡®æ€§
    COMPLETENESS = "completeness"     # å®Œæ•´æ€§
    CONTEXT_RETENTION = "context"     # ä¸Šä¸‹æ–‡ä¿æŒ
    TOOL_USAGE = "tools"              # å·¥å…·ä½¿ç”¨
    LATENCY = "latency"               # å“åº”å»¶è¿Ÿ
    ERROR_HANDLING = "error"          # é”™è¯¯å¤„ç†


@dataclass
class EvaluationCase:
    """è¯„ä¼°ç”¨ä¾‹"""
    id: str
    name: str
    description: str
    dimension: EvaluationDimension
    messages: List[Dict[str, str]]  # å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
    expected_behaviors: List[str]    # æœŸæœ›è¡Œä¸º
    scoring_fn: Optional[Callable] = None  # è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°
    weight: float = 1.0              # æƒé‡


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    case_id: str
    case_name: str
    dimension: str
    score: float  # 0-100
    passed: bool
    latency_ms: float
    details: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)


class ChatBotEvaluator:
    """ChatBot è¯„ä¼°å™¨"""
    
    def __init__(self, backend_url: str = "http://localhost:8000"):
        self.backend_url = backend_url
        self.session = requests.Session()
        self.session.headers.update({"Content-Type": "application/json"})
        self.results: List[EvaluationResult] = []
    
    def check_backend(self) -> bool:
        """æ£€æŸ¥åç«¯æ˜¯å¦å¯ç”¨"""
        try:
            response = self.session.get(f"{self.backend_url}/health", timeout=5)
            return response.status_code in [200, 503]
        except:
            return False
    
    def send_message(self, message: str, session_id: str) -> Dict[str, Any]:
        """å‘é€æ¶ˆæ¯å¹¶è¿”å›å“åº”"""
        start_time = time.time()
        
        try:
            response = self.session.post(
                f"{self.backend_url}/api/v2/chat/message",
                json={"message": message, "session_id": session_id},
                timeout=120
            )
            
            latency = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                data = response.json()
                data["_latency_ms"] = latency
                return data
            else:
                return {
                    "error": f"HTTP {response.status_code}",
                    "message": "",
                    "_latency_ms": latency
                }
        except Exception as e:
            return {
                "error": str(e),
                "message": "",
                "_latency_ms": (time.time() - start_time) * 1000
            }
    
    def evaluate_case(self, case: EvaluationCase) -> EvaluationResult:
        """è¯„ä¼°å•ä¸ªç”¨ä¾‹"""
        session_id = f"eval-{case.id}-{int(time.time())}"
        responses = []
        total_latency = 0
        errors = []
        
        # æ‰§è¡Œå¯¹è¯
        for msg in case.messages:
            response = self.send_message(msg["content"], session_id)
            responses.append(response)
            total_latency += response.get("_latency_ms", 0)
            
            if "error" in response and response["error"]:
                errors.append(response["error"])
        
        # è¯„åˆ†
        if case.scoring_fn:
            score, details = case.scoring_fn(case, responses)
        else:
            score, details = self._default_scoring(case, responses)
        
        return EvaluationResult(
            case_id=case.id,
            case_name=case.name,
            dimension=case.dimension.value,
            score=score,
            passed=score >= 60,
            latency_ms=total_latency / len(case.messages) if case.messages else 0,
            details=details,
            errors=errors
        )
    
    def _default_scoring(
        self, 
        case: EvaluationCase, 
        responses: List[Dict]
    ) -> tuple[float, Dict]:
        """é»˜è®¤è¯„åˆ†é€»è¾‘"""
        score = 0
        details = {"checks": []}
        
        # æ£€æŸ¥æœŸæœ›è¡Œä¸º
        for expected in case.expected_behaviors:
            matched = False
            for resp in responses:
                msg = resp.get("message", "")
                if self._check_behavior(expected, msg, resp):
                    matched = True
                    break
            
            details["checks"].append({
                "expected": expected,
                "matched": matched
            })
            
            if matched:
                score += 100 / len(case.expected_behaviors)
        
        return score, details
    
    def _check_behavior(
        self, 
        expected: str, 
        message: str, 
        response: Dict
    ) -> bool:
        """æ£€æŸ¥æœŸæœ›è¡Œä¸ºæ˜¯å¦æ»¡è¶³"""
        expected_lower = expected.lower()
        message_lower = message.lower()
        
        # å…³é”®è¯åŒ¹é…
        if expected.startswith("contains:"):
            keyword = expected[9:].strip().lower()
            return keyword in message_lower
        
        # é•¿åº¦æ£€æŸ¥
        if expected.startswith("length>"):
            min_len = int(expected[7:])
            return len(message) > min_len
        
        # å·¥å…·ä½¿ç”¨æ£€æŸ¥
        if expected.startswith("used_tool:"):
            tool_name = expected[10:].strip()
            used_tools = response.get("used_tools", [])
            return tool_name in used_tools
        
        # æ— é”™è¯¯æ£€æŸ¥
        if expected == "no_error":
            return "error" not in response or not response["error"]
        
        # é»˜è®¤ï¼šåŒ…å«æ£€æŸ¥
        return expected_lower in message_lower
    
    def run_evaluation(
        self, 
        cases: List[EvaluationCase],
        verbose: bool = True
    ) -> Dict[str, Any]:
        """è¿è¡Œè¯„ä¼°"""
        if not self.check_backend():
            return {"error": "Backend not available"}
        
        self.results = []
        
        for case in cases:
            if verbose:
                print(f"ğŸ“‹ Evaluating: {case.name}...")
            
            result = self.evaluate_case(case)
            self.results.append(result)
            
            if verbose:
                status = "âœ…" if result.passed else "âŒ"
                print(f"   {status} Score: {result.score:.1f}/100, "
                      f"Latency: {result.latency_ms:.0f}ms")
        
        return self.generate_report()
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if not self.results:
            return {"error": "No results"}
        
        # æŒ‰ç»´åº¦åˆ†ç»„
        by_dimension = {}
        for result in self.results:
            dim = result.dimension
            if dim not in by_dimension:
                by_dimension[dim] = []
            by_dimension[dim].append(result)
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        dimension_scores = {}
        for dim, results in by_dimension.items():
            scores = [r.score for r in results]
            dimension_scores[dim] = {
                "mean": statistics.mean(scores),
                "min": min(scores),
                "max": max(scores),
                "pass_rate": sum(1 for r in results if r.passed) / len(results) * 100
            }
        
        # æ€»ä½“ç»Ÿè®¡
        all_scores = [r.score for r in self.results]
        all_latencies = [r.latency_ms for r in self.results]
        
        return {
            "timestamp": datetime.now().isoformat(),
            "total_cases": len(self.results),
            "passed_cases": sum(1 for r in self.results if r.passed),
            "overall_score": statistics.mean(all_scores),
            "overall_pass_rate": sum(1 for r in self.results if r.passed) / len(self.results) * 100,
            "avg_latency_ms": statistics.mean(all_latencies),
            "dimension_scores": dimension_scores,
            "results": [
                {
                    "case_id": r.case_id,
                    "name": r.case_name,
                    "dimension": r.dimension,
                    "score": r.score,
                    "passed": r.passed,
                    "latency_ms": r.latency_ms,
                    "errors": r.errors
                }
                for r in self.results
            ]
        }


# ==================== é¢„å®šä¹‰è¯„ä¼°ç”¨ä¾‹ ====================

STANDARD_EVALUATION_CASES = [
    # ç›¸å…³æ€§æµ‹è¯•
    EvaluationCase(
        id="rel-001",
        name="ç®€å•é—®ç­” - Python å®šä¹‰",
        description="æµ‹è¯•å¯¹åŸºç¡€é—®é¢˜çš„å›ç­”ç›¸å…³æ€§",
        dimension=EvaluationDimension.RELEVANCE,
        messages=[{"role": "user", "content": "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"}],
        expected_behaviors=["contains:python", "contains:ç¼–ç¨‹", "length>50"]
    ),
    EvaluationCase(
        id="rel-002",
        name="ä»£ç ç”Ÿæˆè¯·æ±‚",
        description="æµ‹è¯•ä»£ç ç”Ÿæˆçš„ç›¸å…³æ€§",
        dimension=EvaluationDimension.RELEVANCE,
        messages=[{"role": "user", "content": "å†™ä¸€ä¸ªå†’æ³¡æ’åºçš„ Python ä»£ç "}],
        expected_behaviors=["contains:def", "contains:sort", "contains:for"]
    ),
    
    # ä¸Šä¸‹æ–‡ä¿æŒæµ‹è¯•
    EvaluationCase(
        id="ctx-001",
        name="è®°ä½ç”¨æˆ·åå­—",
        description="æµ‹è¯•æ˜¯å¦è®°ä½ç”¨æˆ·ä¿¡æ¯",
        dimension=EvaluationDimension.CONTEXT_RETENTION,
        messages=[
            {"role": "user", "content": "æˆ‘å«å°æ˜"},
            {"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}
        ],
        expected_behaviors=["contains:å°æ˜"]
    ),
    EvaluationCase(
        id="ctx-002",
        name="è®°ä½è®¨è®ºè¯é¢˜",
        description="æµ‹è¯•æ˜¯å¦ä¿æŒè¯é¢˜è¿ç»­æ€§",
        dimension=EvaluationDimension.CONTEXT_RETENTION,
        messages=[
            {"role": "user", "content": "æˆ‘ä»¬æ¥è®¨è®ºæœºå™¨å­¦ä¹ "},
            {"role": "user", "content": "å®ƒçš„ä¸»è¦åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ"}
        ],
        expected_behaviors=["contains:æœºå™¨å­¦ä¹ ", "no_error"]
    ),
    
    # å‡†ç¡®æ€§æµ‹è¯•
    EvaluationCase(
        id="acc-001",
        name="æ•°å­¦è®¡ç®—",
        description="æµ‹è¯•ç®€å•è®¡ç®—çš„å‡†ç¡®æ€§",
        dimension=EvaluationDimension.ACCURACY,
        messages=[{"role": "user", "content": "2 + 2 ç­‰äºå‡ ï¼Ÿ"}],
        expected_behaviors=["contains:4"]
    ),
    EvaluationCase(
        id="acc-002",
        name="äº‹å®æŸ¥è¯¢",
        description="æµ‹è¯•å¸¸è¯†æ€§é—®é¢˜çš„å‡†ç¡®æ€§",
        dimension=EvaluationDimension.ACCURACY,
        messages=[{"role": "user", "content": "Python æ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ"}],
        expected_behaviors=["contains:1991", "no_error"]
    ),
    
    # é”™è¯¯å¤„ç†æµ‹è¯•
    EvaluationCase(
        id="err-001",
        name="ç©ºæ¶ˆæ¯å¤„ç†",
        description="æµ‹è¯•ç©ºæ¶ˆæ¯çš„å¤„ç†",
        dimension=EvaluationDimension.ERROR_HANDLING,
        messages=[{"role": "user", "content": "   "}],
        expected_behaviors=["no_error", "length>0"]
    ),
    EvaluationCase(
        id="err-002",
        name="æ— æ„ä¹‰è¾“å…¥å¤„ç†",
        description="æµ‹è¯•æ— æ„ä¹‰è¾“å…¥çš„å¤„ç†",
        dimension=EvaluationDimension.ERROR_HANDLING,
        messages=[{"role": "user", "content": "asdfghjkl"}],
        expected_behaviors=["no_error", "length>10"]
    ),
    
    # å®Œæ•´æ€§æµ‹è¯•
    EvaluationCase(
        id="comp-001",
        name="å¤šæ­¥éª¤ä»»åŠ¡è¯´æ˜",
        description="æµ‹è¯•å›ç­”çš„å®Œæ•´æ€§",
        dimension=EvaluationDimension.COMPLETENESS,
        messages=[{"role": "user", "content": "å¦‚ä½•å®‰è£… Pythonï¼Ÿ"}],
        expected_behaviors=["length>100", "no_error"]
    ),
]


def run_standard_evaluation(backend_url: str = "http://localhost:8000"):
    """è¿è¡Œæ ‡å‡†è¯„ä¼°"""
    evaluator = ChatBotEvaluator(backend_url)
    
    print("=" * 60)
    print("ğŸ¤– ChatBot èƒ½åŠ›è¯„ä¼°")
    print("=" * 60)
    print()
    
    report = evaluator.run_evaluation(STANDARD_EVALUATION_CASES)
    
    if "error" in report:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {report['error']}")
        return report
    
    print()
    print("=" * 60)
    print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    print(f"æ€»ç”¨ä¾‹æ•°: {report['total_cases']}")
    print(f"é€šè¿‡æ•°: {report['passed_cases']}")
    print(f"æ€»ä½“å¾—åˆ†: {report['overall_score']:.1f}/100")
    print(f"é€šè¿‡ç‡: {report['overall_pass_rate']:.1f}%")
    print(f"å¹³å‡å»¶è¿Ÿ: {report['avg_latency_ms']:.0f}ms")
    print()
    print("å„ç»´åº¦å¾—åˆ†:")
    for dim, scores in report['dimension_scores'].items():
        print(f"  {dim}: {scores['mean']:.1f}/100 "
              f"(é€šè¿‡ç‡: {scores['pass_rate']:.0f}%)")
    
    return report


if __name__ == "__main__":
    run_standard_evaluation()


